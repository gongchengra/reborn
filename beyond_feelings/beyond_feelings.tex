\documentclass{book}

\title{Beyond Feelings A Guide to Critical Thinking}
\date{2017-10-18}
\author{Vincent Ryan Ruggiero}

\begin{document}
\maketitle
\newpage
\tableofcontents

To the memory of Howard Trumble, whose quiet practice of the skills detailed in this book was an inspiration to me, to his family, and to all who knew him.

\chapter{Preface}

When the first edition of this book appeared in 1975, the dominant intellectual focus was still subjectivity, feelings. That focus, the legacy of the 1960s, was originally a necessary reaction to the rationalism and behaviorism that preceded it. It declared, in effect: "People are not robots. They are more than the sum total of their physiology. They have hopes, dreams, emotions. No two humans are alike-each has a special perspective, a unique way of perceiving the world. And any view of humanity that ignores this subjective side is a distortion."

Yet, despite its value, the focus on feelings went too far. Like many other movements, what began as a reaction against an extreme view became an extreme view itself. The result of that extremism was the neglect of thinking. This book was designed to answer that neglect. The introduction to the first edition explained its rationale as follows:

>The emphasis on subjectivity served to correct a dangerous oversimplification. But it is the kind of reaction that cannot be sustained for long without causing an even worse situation-the neglect of thinking. Worse for two reasons. First, because we live in an age of manipulation. Armies of hucksters and demagogues stand ready with the rich resources of psychology to play upon our emotions and subconscious needs to persuade us that superficial is profound, harmful is beneficial, evil is virtuous. And feelings are especially vulnerable to such manipulation.

>Secondly, because in virtually every important area of modern life-law, medicine, government, education, science, business, and community affairs-we are beset with serious problems and complex issues that demand careful gathering and weighing of facts and informed opinions, thoughtful consideration of various conclusions or actions, and judicious selection of the best conclusion or most appropriate action. . . .

>[Today’s college student] has been conditioned not to undervalue subjectivity, but to overvalue it. And so he does not need to have his feelings indulged. Rather, he needs to be taught how to sort out his feelings, decide to what extent they have been shaped by external influences, and evaluate them carefully when they conflict among themselves or with the feelings of others. In short, he needs to be taught to think critically.

>There is an unfortunate tendency among many to view feeling and thought as mutually exclusive, to force a choice between them. If we focus on one, then in their view we must reject the other. But this is mistaken. Feeling and thought are perfectly complementary. Feeling, being more spontaneous, is an excellent beginning to the development of conclusions. And thought, being more deliberate, provides a way to identify the best and most appropriate feeling. Both are natural.

>Thinking, however, is less automatic than feeling. To do it well demands a systematic approach and guided practice.

The general attitude toward thinking has changed considerably since the mid-1970s. The view that critical thinking is an important skill to which education should give prominence is no longer a minority view. Hundreds of voices have joined the chorus calling for the addition of critical thinking objectives to existing courses and even the creation of special courses in thinking. There is little disagreement that the challenges of the new millennium demand minds that can move beyond feelings to clear, impartial, critical problem solving and decision making.

\section{Features of This Edition}

This edition of Beyond Feelings retains the basic organization of previous editions. The first section explains the psychological, philosophical, and social context in which critical thinking takes place and describes the habits and attitudes that enhance such thinking. The second section helps students recognize and overcome common errors in thinking. The third section provides a step-by-step strategy for dealing with issues.

Within the overall design, however, I have made a number of changes, most in response to the helpful suggestions of reviewers.
In Chapter 1, a new section-"The Influence of Ideas"-has been added.
In Chapter 3, a new section-"Understanding Cause and Effect"- has been added.
In Chapter 15, new examples of the value of observation have been added.
In Chapter 17, the subsection "Evaluate your information sources" has been expanded.
A number of new "Difference of Opinion" exercises have been added.

As in the past, I have attempted to follow George Orwell’s sage advice: "Never use a foreign phrase, a scientific word, or a jargon word if you can think of an everyday English equivalent." This is not always easy. When logicians are taught terms such as argumentum ad hominem, non sequitur, and "affirming the consequent," they naturally want to use them. Arguments for doing so urge themselves upon us: for example, "These are the most precise terms. Don’t join the ranks of the coddlers and deprive students of them." In weak moments I succumb to this appeal. (Until the previous edition, for example, I included the term enthymeme. Mea culpa . . . there I go again.) But is the precision of such terms the real reason for my wanting to use them? Is it not possible that we professors enjoy parading our knowledge or that we are reluctant to spare our students the struggle we were forced to undergo ("We suffered, so they should too")? It seems to me that modern culture already provides too many impediments to critical thinking for us to add more.

Is it possible to carry this plain language commitment too far? Yes, and some will think I have done so in avoiding the term inferences and speaking instead of conclusions. But I respectfully disagree. Lexicographers point out that the distinction between these terms is extremely subtle, so it seems more reasonable not to devote time to it. Also, I avoid using the term values whenever possible for a somewhat different reason. The word value is so associated with relativism that its use in this context can undermine the crucial idea that arguments differ in quality. For many students, the word value triggers the thought, "Everyone has a right to his or her values; mine are right for me, and though they may need ‘clarification’ from time to time, they are never to be questioned." This thought impedes critical thinking.

\chapter{Introduction}

Beyond Feelings is designed to introduce you to the subject of critical thinking. The subject may be new to you because it has not been emphasized in most elementary and secondary schools. In fact, until fairly recently, most colleges gave it little attention. For the past four decades, the dominant emphasis has been on subjectivity rather than objectivity, on feeling rather than on thought.

Over the past several decades, however, a number of studies of America’s schools have criticized the neglect of critical thinking, and a growing number of educators and leaders in business, industry, and the professions have urged the development of new courses and teaching materials to overcome that neglect.

It is no exaggeration to say that critical thinking is one of the most important subjects you will study in college regardless of your academic major. The quality of your schoolwork, your efforts in your career, your contributions to community life, your conduct of personal affairs-all will depend on your ability to solve problems and make decisions.

The book has three main sections. The first, "The Context," will help you understand such important concepts as individuality, critical thinking, truth, knowledge, opinion, evidence, and argument and overcome attitudes and ideas that obstruct critical thinking. The second section, "The Pitfalls," will teach you to recognize and avoid the most common errors in thinking. The third section, "A Strategy," will help you acquire the various skills used in addressing problems and issues. This section includes tips on identifying and overcoming your personal intellectual weaknesses as well as techniques for becoming more observant, clarifying issues, conducting inquiries, evaluating evidence, analyzing other people’s views, and making sound judgments.

At the end of each chapter, you will find a number of applications to challenge your critical thinking and help you exercise your skills. These applications cover problems and issues both timely and timeless. The final application in each of the first thirteen chapters invites you to examine an especially important issue about which informed opinion is divided.

Students sometimes get the idea that a textbook must be read page by page and that reading ahead violates some unwritten rule. This notion is mistaken. Students’ background knowledge varies widely; what one student knows very well, another knows only vaguely and a third is totally unfamiliar with. Any time you need or want to look ahead to an explanation in a later chapter, by all means do so. Let’s say you make a statement and a friend says, "That’s relativism, pure and simple." If you aren’t sure exactly what she means, go to the index, look up "relativism," proceed to the appropriate page, and find out.

Looking ahead is especially prudent in the case of concepts and procedures relevant to the end-of-chapter applications. One such concept is plagiarism. If you are not completely clear on what constitutes plagiarism, why it is unacceptable, and how to avoid it, take a few minutes right now to learn. Look for the section "Avoiding Plagiarism" toward the end of the Chapter 2. Similarly, if you are not as skilled as you would like to be doing library or Internet research, it would be a good idea to read Chapter 17 now. Doing so could save you a great deal of time and effort completing homework assignments.

\part{The Context}

Anyone who wishes to master an activity must first understand its tools and rules. This is as true of critical thinking as it is of golf, carpentry, flying a plane, or brain surgery. In critical thinking, however, the tools are not material objects but concepts, and the rules govern mental rather than physical performance.

This first section explores seven important concepts-individuality, critical thinking, truth, knowledge, opinion, evidence, and argument- with a chapter devoted to each. Most of these concepts are so familiar that you may be inclined to wonder whether there is any point to examining them. The answer is yes, for three reasons. First, much of what is commonly believed about these concepts is mistaken. Second, who ever examines them carefully is always rewarded with fresh insights. Third, the more thorough your knowledge of these concepts, the more proficient you will be in your thinking.

\chapter{CHAPTER 1 Who Are You?}

Suppose someone asked, "Who are you?" It would be simple enough to respond with your name. But if the person wanted to know the entire story about who you are, the question would be more difficult to answer. You’d obviously have to give the details of your height, age, and weight. You’d also have to include all your sentiments and preferences, even the secret ones you’ve never shared with anyone-your affection for your loved ones; your desire to please the people you associate with; your dislike of your older sister’s husband; your allegiance to your favorite beverage, brand of clothing, and music.

Your attitudes couldn’t be overlooked either-your impatience when an issue gets complex, your aversion to certain courses, your fear of high places and dogs and speaking in public. The list would go on. To be complete, it would have to include all your characteristics-not only the physical but also the emotional and intellectual.

To provide all that information would be quite a chore. But suppose the questioner was still curious and asked, "How did you get the way you are?" If your patience were not yet exhausted, chances are you’d answer something like this: "I’m this way because I choose to be, because I’ve considered other sentiments and preferences and attitudes and have made my selections. The ones I have chosen fit my style and personality best." That answer is natural enough, and in part it’s true. But in a larger sense, it’s not true. The impact of the world on all of us is much greater than most of us realize.

\section{The Influence of Time and Place}

Not only are you a member of a particular species, Homo sapiens, but you also exist at a particular time in the history of that species and in a particular place on the planet. That time and place are defined by specific circumstances, understandings, beliefs, and customs, all of which limit your experience and influence your thought patterns. If you had lived in America in colonial times, you likely would have had no objection to the practice of barring women from serving on a jury, entering into a legal contract, owning property, or voting. If you had lived in the nineteenth century, you would have had no objection to young children being denied an education and being hired out by their parents to work sixteen hours a day, nor would you have given any thought to the special needs of adolescence. (The concept of adolescence was not invented until 1904.)

If you had been raised in the Middle East, you would stand much closer to people you converse with than you do in America. If you had been raised in India, you might be perfectly comfortable having your parents choose your spouse for you. If your native language were Spanish and your knowledge of English modest, you probably would be confused by some English colloquialisms. James Henslin offers two amusing examples of such confusion: Chevrolet Novas initially sold very poorly in Mexico because no va in Spanish means "it doesn’t work"; and Perdue chickens were regarded with a certain suspicion (or worse) because the company’s slogan-"It takes a tough man to make a tender chicken"- became in Spanish "It takes an aroused man to make a chicken affectionate."

People who grow up in Europe, Asia, or South America have very different ideas of punctuality. As Daniel Goleman explains, "Five minutes is late but permissible for a business appointment in the U.S., but thirty minutes is normal in Arab countries. In England five to fifteen minutes is the ‘correct’ lateness for one invited to dinner; an Italian might come two hours late, an Ethiopian still later, a Javanese not at all, having accepted only to prevent his host’s losing face." A different ethnic origin would also mean different tastes in food. Instead of craving a New York Strip steak and french fries, you might crave "raw monkey brains" or "camel’s milk cheese patties cured in dry camel’s dung" and washed down with "warm camel’s blood." Sociologist Ian Robertson summed up the range of global dietary differences succinctly: "Americans eat oysters but not snails. The French eat snails but not locusts. The Zulus eat locusts but not fish. The Jews eat fish but not pork. The Hindus eat pork but not beef. The Russians eat beef but not snakes. The Chinese eat snakes but not people. The Jalé of New Guinea find people delicious." [Note: The reference to Hindus is mistaken.]

To sum up, living in a different age or culture would make you a different person. Even if you rebelled against the values of your time and place, they still would represent the context of your life-in other words, they still would influence your responses.

\section{The Influence of Ideas}

When one idea is expressed, closely related ideas are simultaneously conveyed, logically and inescapably. In logic, this kinship is expressed by the term sequitur, Latin for "it follows." (The converse is non sequitur, "it does not follow.")

Consider, for example, the idea that many teachers and parents express to young children as a way of encouraging them: "If you believe in yourself, you can succeed at anything." From this it follows that nothing else but belief-neither talent nor hard work-is necessary for success. The reason the two ideas are equivalent is that their meanings are inseparably linked.

In addition to conveying ideas closely linked to it in meaning, an idea can imply other ideas. For example, the idea that there is no real difference between virtue and vice implies that people should not feel bound by common moral standards. Samuel Johnson had this implication in mind when he said: "But if he does really think that there is no distinction between virtue and vice, why, Sir, when he leaves our houses let us count our spoons."

If we were fully aware of the closely linked meanings and implications of the ideas we encounter, we could easily sort out the sound ones from the unsound, the wise from the foolish, and the helpful from the harmful. But we are seldom fully aware. In many cases, we take ideas at face value and embrace them with little or no thought of their associated meanings and implications. In the course of time, our actions are shaped by those meanings and implications, whether we are aware of them or not.

To appreciate the influence of ideas in people’s lives, consider the series of events set in motion by an idea that was popular in psychology more than a century ago and whose influence continues to this day-the idea that "intelligenceisgeneticallydeterminedandcannotbeincreased."

> That idea led researchers to devise tests that measure intelligence. The most famous (badly flawed) test determined that the average mental age of white American adults was 13 and that, among immigrants, the average Russian’s mental age was 11.34; the average Italian’s, 11.01; the average Pole’s, 10.74; and the average mental age of "Negroes," 10.41.

> Educators read the text results and thought, "Attempts to raise students’ intelligence are pointless," so they replaced academic curricula with vocational curricula and embraced a methodology that taught students facts but not the process of judgment.

> Legislators read the test results and decided "We’ve got to do something to keep intellectually inferior people from entering the country," so they revised immigration laws to discriminate against southern and central Europeans.

> Eugenicists, who had long been concerned about the welfare of the human species,saw the tests as a grave warning.They thought, "If intelligence can not be increased, we must find ways of encouraging reproduction among people of higher intelligence and discouraging it among those of lower intelligence."

> The eugenicists’ concern inspired a variety of actions. Margaret Sanger’s Planned Parenthood urged the lower classes to practice contraception. Others succeeded in legalizing promoted forced sterilization, notably in Virginia. The U.S. Supreme Court upheld the Virginia law with Justice Oliver Wendell Holmes, Jr. declaring, "Three generations of imbeciles are enough." Over the next five decades 7,500 women, including "unwed mothers, prostitutes, petty criminals and children with disciplinary problems" were sterilized. In addition, by 1950 over 150,000 supposedly "defective" children, many relatively normal, were held against their will in institutions. They "endured isolation, overcrowding, forced labor, and physical abuse including lobotomy, electroshock, and surgical sterilization."

> Meanwhile, business leaders read the test results and decided, "We need policies to ensure that workers leave their minds at the factory gate and perform their assigned tasks mindlessly." So they enacted those policies. Decades later,when Edwards Deming proposed his "quality control" ideas for involving workers in decision making, business leaders remembered those test results and ignored Deming’s advice. (In contrast, the Japanese welcomed Deming’s ideas; as a result, several of their industries surged ahead of their American competition.)

These are the most obvious effects of hereditarianism but they are certainly not the only ones. Others include discrimination against racial and ethnic minorities and the often-paternalistic policies of government offered in response. (Some historians also link hereditarianism to the genocide that occurred in Nazi Germany.)

The innumerable ideas you have encountered will affect your beliefs and behavior in similar ways––sometimes slightly, at other times profoundly. And this can happen even if you have not consciously embraced the ideas.

\section{The Influence of Mass Culture}

In centuries past, family and teachers were the dominant, and sometimes the only, influence on children. Today, however, the influence exerted by mass culture (the broadcast media, newspapers, magazines, Internet and popular music) often is greater.

By age 18 the average teenager has spent 11,000 hours in the classroom and 22,000 hours in front of the television set. He or she has had perhaps 13,000 school lessons yet has watched more than 750,000 commercials. By age thirty-five the same person has had fewer than 20,000 school lessons yet has watched approximately 45,000 hours of television and close to 2 million commercials.

What effects does mass culture have on us? To answer, we need only consider the formats and devices commonly used in the media. Modern advertising typically bombards the public with slogans and testimonials by celebrities. This approach is designed to appeal to emotions and create artificial needs for products and services. As a result, many people develop the habit of responding emotionally, impulsively, and gullibly to such appeals. They also tend to acquire values very different from those taught in the home and the school. Ads often portray play as more fulfilling than work, self-gratification as more desirable than self-control, and materialism as more meaningful than idealism.

Television programmers use frequent scene shifts and sensory appeals such as car crashes, violence, and sexual encounters to keep audience interest from diminishing. Then they add frequent commercial interruptions. This author has analyzed the attention shifts that television viewers are subjected to. In a dramatic program, for example, attention shifts might include camera angle changes; shifts in story line from one set of characters (or subplot) to another, or from a present scene to a past scene (flashback), or to fantasy; and shifts to "newsbreaks," to commercial breaks, from one commercial to another, and back to the program. Also included might be shifts of attention that occur within commercials. I found as many as 78 shifts per hour, excluding the shifts within commercials. The number of shifts within commercials ranged from 6 to 54 and averaged approximately 17 per fifteen-second commercial. The total number of attention shifts came out to over 800 per hour, or over 14 per minute.

This manipulation has prevented many people from developing a mature attention span. They expect the classroom and the workplace to provide the same constant excitement they get from television. That, of course, is an impossible demand, and when it isn’t met they call their teachers boring and their work unfulfilling. Because such people seldom have the patience to read books that require them to think, many publishers have replaced serious books with light fare written by celebrities.

Even when writers of serious books do manage to become published authors, they are often directed to give short, dramatic answers during promotional interviews, sometimes at the expense of accuracy. A man who coaches writers for talk shows offered one client this advice: "If I ask you whether the budget deficit is a good thing or a bad thing, you should not say, ‘Well, it stimulates the economy but it passes on a burden.’ You have to say ‘It’s a great idea!’ or ‘It’s a terrible idea!’ It doesn’t matter which." (Translation: "Don’t give a balanced answer. Give an oversimplified one because it will get you noticed.")

Print journalism is also in the grip of sensationalism. As a newspaper editor observed, "Journalists keep trying to find people who are at 1 or at 9 on a scale of 1 to 10 rather than people at 3 to 7 [the more moderate positions] where most people actually are." Another journalist claims, "News is now becoming more opinion than verified fact. Journalists are slipping into entertainment rather than telling us the verified facts we need to know."

Today’s politicians often manipulate people more offensively than do journalists. Instead of expressing their thoughts, some politicians find out what people think and pretend to share their ideas. Many politicians hire people to conduct polls and focus groups to learn what messages will "sell." They even go so far as to test the impact of certain words-that is why we hear so much about "trust," "family," "character," and "values" these days. Political science professor Larry Sabato says that during the Clinton impeachment trial, the president’s advisors used the term private lives over and over-James Carville used it six times in one four-minute speech-because they knew it could persuade people into believing the president’s lying under oath was of no great consequence.

\section{The "Science" of Manipulation}

Attempts to influence the thoughts and actions of others are no doubt as old as time, but manipulation did not become a science until the early twentieth century, when Ivan Pavlov, a Russian professor of psychology, published his research on conditioned (learned) reflexes. Pavlov found that by ringing a bell when he fed a dog, he could condition the dog to drool at the sound of the bell even when no food was presented. An American psychologist, John Watson, was impressed with Pavlov’s findings and applied them to human behavior. In Watson’s most famous experiment, he let a baby touch a laboratory rat. At first, the baby was unafraid. But then Watson hit a hammer against metal whenever the baby reached out to touch the rat, and the baby became frightened and cried. In time, the baby cried not only at the sight of the rat but also at the sight of anything furry, such as a stuffed animal. Watson’s work earned him the title "father of behaviorism."

Less well known is Watson’s application of behaviorist principles to advertising. He spent the latter part of his career working for advertising agencies and soon recognized that the most effective appeal to consumers was not to the mind but to the emotions. He advised advertisers to "tell [the consumer] something that will tie him up with fear, something that will stir up a mild rage, that will call out an affectionate or love response, or strike at a deep psychological or habit need." His attitude toward the consumer is perhaps best indicated by a statement he made in a presentation to department store executives: "The consumer is to the manufacturer, the department stores and the advertising agencies, what the green frog is to the physiologist."

Watson introduced these strategies in the 1920s and 1930s, the age of newspapers and radio. Since the advent of television, these advertising strategies have grown more sophisticated and effective, so much so that many individuals and groups with political and social agendas have adopted them. The strategies work for a number of reasons, the chief one being people’s conviction that they are impervious to manipulation. This belief is mistaken, as many researchers have demonstrated. For example, Solomon Asch showed that people’s reactions can be altered simply by changing the order of words in a series. He asked study participants to evaluate a person by a series of adjectives. When he put positive adjectives first-"intelligent, industrious, impulsive, critical, stubborn, envious"- the participants gave a positive evaluation. When he reversed the order, with "envious" coming first and "intelligent" last, they gave a negative evaluation.

Similarly, research has shown that human memory can be manipulated. The way a question is asked can change the details in a person’s memory and even make a person remember something that never happened!

Of course, advertisers and people with political or social agendas are not content to stimulate emotions and/or plant ideas in our minds. They also seek to reinforce those impressions by repeating them again and again. The more people hear a slogan or talking point, the more familiar it becomes. Before long, it becomes indistinguishable from ideas developed through careful thought. Sadly, "the packaging is often done so effectively that the viewer, listener, or reader does not make up his own mind at all. Instead, he inserts a packaged opinion into his mind, somewhat like inserting a DVD into a DVD player. He then pushes a button and ‘plays back’ the opinion whenever it seems appropriate to do so. He has performed acceptably without having had to think." Many of the beliefs we hold dearest and defend most vigorously may have been planted in our minds in just this way.

Many years ago, Harry A. Overstreet noted that "a climate of opinion, like a physical climate, is so pervasive a thing that those who live within it and know no other take it for granted." The rise of mass culture and the sophisticated use of manipulation have made this insight more relevant today than ever.

\section{The Influence of Psychology}

The social and psychological theories of our time also have an impact on our beliefs. Before the past few decades, people were urged to be self-disciplined, self-critical, and self-effacing. They were urged to practice self-denial, to aspire to self-knowledge, to behave in a manner that ensured they maintained self-respect. Self-centeredness was considered a vice. "Hard work," they were told, "leads to achievement, and that in turn produces satisfaction and self-confidence." By and large, our grandparents internalized those teachings. When they honored them in their behavior, they felt proud; when they dishonored them, they felt ashamed.

Today the theories have been changed-indeed, almost exactly reversed. Self-esteem, which nineteenth-century satirist Ambrose Bierce defined as "an erroneous appraisement," is now considered an imperative. Self-centeredness has been transformed from vice into virtue, and people who devote their lives to helping others, people once considered heroic and saintlike, are now said to be afflicted with "a disease to please." The formula for success and happiness begins with feeling good about ourselves. Students who do poorly in school, workers who don’t measure up to the challenges of their jobs, substance abusers, lawbreakers-all are typically diagnosed as deficient in self-esteem.

In addition, just as our grandparents internalized the social and psychological theories of their time, so most contemporary Americans have internalized the message of self-esteem. We hear people speak of it over coffee; we hear it endlessly invoked on talk shows. Challenges to its precepts are usually met with disapproval.

But isn’t the theory of self-esteem self-evident? No. A negative perception of our abilities will, of course, handicap our performance. Dr. Maxwell Maltz explains the amazing results one educator had in improving the grades of schoolchildren by changing their self-images. The educator had observed that when the children saw themselves as stupid in a particular subject (or stupid in general), they unconsciously acted to confirm their self-images. They believed they were stupid, so they acted that way. Reasoning that it was their defeatist attitude rather than any lack of ability that was undermining their efforts, the educator set out to change their self-images. He found that when he accomplished that, they no longer behaved stupidly! Maltz concludes from this and other examples that our experiences can work a kind of self-hypnotism on us, suggesting a conclusion about ourselves and then urging us to make it come true.

Many proponents of self-esteem went far beyond Maltz’s demonstration that self-confidence is an important ingredient in success. They claimed that there is no such thing as too much self-esteem. Research does not support that claim. For example, Martin Seligman, an eminent research psychologist and founder of the movement known as positive psychology, cites significant evidence that, rather than solving personal and social problems, including depression, the modern emphasis on self-esteem causes them.

Maltz’s research documents that lack of confidence impedes performance, a valuable insight. But such research doesn’t explain why the more global concept of self-esteem has become so dominant. The answer to that question lies in the popularization of the work of humanistic psychologists such as Abraham Maslow. Maslow described what he called the hierarchy of human needs in the form of a pyramid, with physiological needs (food and drink) at the foundation. Above them, in ascending order, are safety needs, the need for belongingness and love, the need for esteem and approval, and aesthetic and cognitive needs (knowledge, understanding, etc.). At the pinnacle is the need for self-actualization, or fulfillment of our potential. In Maslow’s view, the lower needs must be fulfilled before the higher ones. It’s easy to see how the idea that self-esteem must precede achievement was derived from Maslow’s theory.

Other theories might have been adopted, however. A notable one is Austrian psychiatrist Viktor Frankl’s, which was advanced at roughly the same time as Maslow’s and was based on both Frankl’s professional practice and his experiences in Hitler’s concentration camps. Frankl argues that one human need is higher than self-actualization: self-transcendence, the need to rise above narrow absorption with self. According to Frankl, "the primordial anthropological fact [is] that being human is being always directed, and pointing to something or someone other than oneself: to a meaning to fulfill or another human being to encounter, a cause to serve or a person to love." A person becomes fully human "by forgetting himself and giving himself, overlooking himself and focusing outward."

Making self-actualization (or happiness) the direct object of our pursuit, in Frankl’s view, is ultimately self-defeating; such fulfillment can occur only as "the unintended effect of self-transcendence." The proper perspective on life, Frankl believes, is not what it can give to us, but what it expects from us; life is daily-even hourly-questioning us, challenging us to accept "the responsibility to find the right answer to its problems and to fulfill the tasks which it constantly sets for [each of us]."

Finding meaning, according to Frankl’s theory, involves "perceiving a possibility embedded in reality" and searching for challenging tasks "whose completion might add meaning to [one’s] existence." But such perceiving and searching are frustrated by the focus on self: "As long as modern literature confines itself to, and contents itself with, self-expression-not to say self-exhibition-it reflects its authors’ sense of futility and absurdity. What is more important, it also creates absurdity. This is understandable in light of the fact that meaning must be discovered, it cannot be invented. Sense cannot be created, but what may well be created is nonsense."

Whether we agree completely with Frankl, one thing is clear: Contemporary American culture would be markedly different if the emphasis over the past several decades had been on Frankl’s theory rather than on the theories of Maslow and the other humanistic psychologists. All of us would have been affected-we can only imagine how profoundly-in our attitudes, values, and beliefs.

\section{Becoming an Individual}

In light of what we have discussed, we should regard individuality not as something we are born with but rather as something acquired-or, more precisely, earned. Individuality begins in the realization that it is impossible to escape being influenced by other people and by circumstance. The essence of individuality is vigilance. The following guidelines will help you achieve this:

1. Treat your first reaction to any person, issue, or situation as tentative. No matter how appealing it may be, refuse to embrace it until you have examined it.
2. Decide why you reacted as you did. Consider whether you borrowed the reaction from someone else-a parent or friend, perhaps, or a celebrity or fictional character on television. If possible, determine what specific experiences conditioned you to react this way.
3. Think of other possible reactions you might have had to the person, issue, or situation.
4. Ask yourself whether one of the other reactions is more appropriate than your first reaction. And when you answer, resist the influence of your conditioning.

To ensure that you will really be an individual and not merely claim to be one, apply these guidelines throughout your work in this book, as well as in your everyday life.

\section{Applications}

Note: One of the best ways to develop your thinking (and writing) skills is to record your observations, questions, and ideas in a journal and then, as time permits, to reflect on what you have recorded-considering the meaning and application of the observations, answering the questions, elaborating on the ideas (and, where appropriate, challenging them), and recording your insights. An inexpensive bound notebook or spiral notebook will serve the purpose. A good approach is to record your initial observations, questions, and ideas on the left side of the page, leaving the right side blank for your later analysis and commentary. The value of this reflective process is so great that you should consider keeping such a journal even if your instructor does not make it a formal part of the course.

1. Do a brief study of attention shifts such as the one described in the chapter. Record a half-hour show. Then play the show back twice, the first time counting the number of shifts within the program, excluding commercials, and the second time counting only those within commercials. Complete the necessary arithmetic and be prepared to share your results in class.
2. Reflect on your findings in application 1.Write several paragraphs discussing the implications of those findings for education, business, and family life.
3. Many people cheerfully pay \$6 or \$7 a gallon for designer drinking water but moan and groan when they have to pay \$3 a gallon for gasoline. Does anything you read in this chapter help you understand why this is so?
4. Imagine how different America might beif Frankl’s emphasis on self-transcendence and personal responsibility, rather than Maslow’s emphasis on self-actualization and popular culture’s emphasis on self-esteem, had been dominant for the past fifty years. List as many ways as you can in which our society might be different today and comment on whether each would be beneficial or harmful. Be prepared to explain your views in class discussion.
5. Watch one of the music video channels-MTV,VH1,CMT,BET-for at least an hour. Analyze how men and women are depicted in the videos. Note significant details. For example, observe whether men are depicted in power roles more than women and whether women are portrayed as objects of male desire. Decide what attitudes and values are conveyed. (You might want to record as you are watching so that you can review what you have seen, freeze significant frames for closer analysis, and keep your observations for later reference or class viewing and discussion.)
6. Suppose you asked a friend, "How did you acquire your particular identity-your sentiments and preferences and attitudes?" Then suppose the friend responded, "I’m an individual. No one else influences me. I do my own thing, and I select the sentiments and preferences and attitudes that suit me." How would you explain to your friend what you learned in this chapter?
7. Ask yourself the question,Who am I? Write down ten answers to this question, each on a separate slip of paper. Use the first three paragraphs of this chapter to help you frame your answers. Arrange the pieces of paper in order of their importance to you. Then explain the arrangement-that is, which selfdescriptions are most important to you, and why?
8. Identify the various positive and negative influences that have shaped you. Be sure to include the particular as well as the general and the subtle as well as the obvious influences. Which of those influences have had the greatest effect on you? Explain the effects as precisely as you can.
9. Note your immediate reaction to each of the following statements. Then apply the four guidelines given in this chapter for achieving individuality.
    a. Health care workers should be required to be tested for HIV/AIDS.
    b. Beauty contests and talent competitions for children should be banned.
    c. Extremist groups like the Ku Klux Klan should be allowed to hold rallies on public property or be issued permits to hold parades on city streets.
    d. Freshman composition should be a required course for all students.
    e. High school and college athletes should be tested for anabolic steroid use.
    f. Creationism should be taught in high school biology classes.
    g. Polygamy should be legalized.
    h. The voting age should be lowered to sixteen.
    i. The prison system should give greater emphasis to the punishment of inmates than to their rehabilitation.
    j. Doctors and clinics should be required to notify parents of minors when they prescribe birth control devices or facilitate abortions for the minors.
    k. A man’s self-esteem is severely injured if his wife makes more money than he makes.
    l. Women like being dependent on men.
10. Group discussion exercise: Discuss several of the statements in application 9 with two or three of your classmates, applying the four guidelines presented in this chapter for developing individuality. Be prepared to share your group’s ideas with the class.

\section{A Difference of Opinion}

The following passage summarizes an important difference of opinion. After reading the statement, use the library and/or the Internet and find what knowledgeable people have said about the issue. Be sure to cover the entire range of views. Then assess the strengths and weaknesses of each. If you conclude that one view is entirely correct and the others are mistaken, explain how you reached that conclusion. If, as is more likely, you find that one view is more insightful than the others but that they all make some valid points, construct a view of your own that combines insights from all views and explain why that view is the most reasonable of all. Present your response in a composition or an oral report, as your instructor specifies.

> Should captured terrorists be tried in military or criminal courts? When the United States decided to use the military base at Guantanamo Bay, Cuba, to detain individuals captured on the battlefield in the Iraq war, many people protested the decision. Some argued that captured individuals should be considered criminals rather than prisoners of war and accorded the rights guaranteed by the U.S. Constitution to all people accused of crimes. Others argued for classifying the individuals as prisoners of war and treating them as specified in the Geneva Conventions of 1949. Supporters of the government’s decision reject both arguments, contending that captured terrorists are neither criminals nor soldiers but "unlawful combatants," adding that any other designation would impose burdens on the United States that would make it difficult to fight terrorism and thereby threaten national security.

Begin your analysis by conducting a Google search using the term "status captured terrorists."

\chapter{What Is Critical Thinking?}

When Arthur was in the first grade, the teacher directed the class to "think." "Now, class," she said, "I know this problem is a little harder than the ones we’ve been doing, but I’m going to give you a few extra minutes to think about it. Now start thinking."

It was not the first time Arthur had heard the word used. He’d heard it many times at home, but never quite this way. The teacher seemed to be asking for some special activity, something he should know how to start and stop-like his father’s car. "Vroom-m-m," he muttered half aloud. Because of his confusion, he was unaware he was making the noise.

"Arthur, please stop making noises and start thinking."

Embarrassed and not knowing quite what to do, he looked down at his desk. Then, out of the corner of his eye, he noticed that the little girl next to him was staring at the ceiling. "Maybe that’s the way you start thinking," he guessed. He decided the others had probably learned how to do it last year, that time he was home with the measles. So he stared at the ceiling.

As he progressed through grade school and high school, he heard that same direction hundreds of times. "No, that’s not the answer, you’re not thinking-now think!" And occasionally he would hear from particularly self-pitying teachers given to muttering to themselves aloud: "What did I do to deserve this? Don’t they teach them anything in the grades anymore? Don’t you people care about ideas? Think, dammit, THINK."

So Arthur learned to feel somewhat guilty about the whole matter. Obviously, this thinking was an important activity that he’d failed to learn. Maybe he lacked the brain power. But he was resourceful enough. He watched the other students and did what they did. Whenever a teacher started in about thinking, he screwed up his face, furrowed his brow, scratched his head, stroked his chin, stared off into space or up at the ceiling, and repeated silently to himself, "Let’s see now, I’ve got to think about that, think, think-I hope he doesn’t call on me-think." Though Arthur didn’t know it, that’s just what the other students were saying to themselves.

Your experience may have been similar to Arthur’s. In other words, many people may have simply told you to think without ever explaining what thinking is and what qualities a good thinker has that a poor thinker lacks. If that is the case, you have a lot of company. Extensive, effective training in thinking is the exception rather than the rule. This fact and its unfortunate consequences are suggested by the following comments from accomplished observers of the human condition:

> The most interesting and astounding contradiction in life is to me the constant insistence by nearly all people upon "logic," "logical reasoning," "sound reasoning," on the one hand, and on the other their inability to display it, and their unwillingness to accept it when displayed by others.
> Most of our so-called reasoning consists in finding arguments for going on believing as we already do.
> Clear thinking is a very rare thing, but even just plain thinking is almost as rare. Most of us most of the time do not think at all. We believe and we feel, but we do not think.
> Mental indolence is one of the commonest of human traits.

What is this activity that everyone claims is important but few people have mastered? Thinking is a general term used to cover numerous activities, from daydreaming to reflection and analysis. Here are just some of the synonyms listed in Roget’s Thesaurus for think:

appreciate believe cerebrate cogitate conceive consider
consult contemplate deliberate digest discuss dream
fancy imagine meditate muse ponder realize
reason reflect ruminate speculate suppose weigh

All of those are just the names that thinking goes under. They really don’t explain it. The fact is, after thousands of years of humans’ experiencing thought and talking and writing about thinking, it remains in many respects one of the great mysteries of our existence. Still, though much is yet to be learned, a great deal is already known.

\section{Mind, Brain, or Both?}

Most modern researchers use the word mind synonymously with brain, as if the physical organ that resides in the human skull were solely responsible for thinking. This practice conveniently presupposes that a problem that has challenged the greatest thinkers for millennia-the relationship between mind and physical matter-was somehow solved when no one was looking. The problem itself and the individuals who spent their lives wrestling with it deserve better.

Neuroscience has provided a number of valuable insights into the cognitive or thinking activities of the brain. It has documented that the left hemisphere of the brain deals mainly with detailed language processing and is associated with analysis and logical thinking, that the right hemisphere deals mainly with sensory images and is associated with intuition and creative thinking, and that the small bundle of nerves that lies between the hemispheres-the corpus callosum-integrates the various functions.

The research that produced these insights showed that the brain is necessary for thought, but it has not shown that the brain is sufficient for thought. In fact, many philosophers claim it can never show that. They argue that the mind and the brain are demonstrably different. Whereas the brain is a physical entity composed of matter and therefore subject to decay, the mind is a metaphysical entity. Examine brain cells under the most powerful microscope and you will never see an idea or concept- for example, beauty, government, equality, or love-because ideas and concepts are not material entities and so have no physical dimension. Where, then, do these nonmaterial things reside? In the nonmaterial mind.

The late American philosopher William Barrett observed that "history is, fundamentally, the adventure of human consciousness" and "the fundamental history of humankind is the history of mind." In his view, "one of the supreme ironies of modern history" is the fact that science, which owes its very existence to the human mind, has had the audacity to deny the reality of the mind. As he put it, "the offspring denies the parent."

The argument over whether the mind is a reality is not the only issue about the mind that has been hotly debated over the centuries. One especially important issue is whether the mind is passive, a blank slate on which experience writes, as John Locke held, or active, a vehicle by which we take the initiative and exercise our free will, as G. W. Leibnitz argued. This book is based on the latter view.

\section{Critical Thinking Defined}

Let’s begin by making the important distinction between thinking and feeling. I feel and I think are sometimes used interchangeably, but that practice causes confusion. Feeling is a subjective response that reflects emotion, sentiment, or desire; it generally occurs spontaneously rather than through a conscious mental act. We don’t have to employ our minds to feel angry when we are insulted, afraid when we are threatened, or compassionate when we see a picture of a starving child. The feelings arise automatically.

Feeling is useful in directing our attention to matters we should think about; it also can provide the enthusiasm and commitment necessary to complete arduous mental tasks. However, feeling is never a good substitute for thinking because it is notoriously unreliable. Some feelings are beneficial, honorable, even noble; others are not, as everyday experience demonstrates. We often feel like doing things that will harm us-for example, smoking, sunbathing without sunscreen, telling off our professor or employer, or spending the rent money on lottery tickets.

Zinedine Zidane was one of the greatest soccer players of his generation, and many experts believed that in his final season (2006) he would lead France to the pinnacle of soccer success-winning the coveted World Cup. But then, toward the end of the championship game against Italy, he viciously head-butted an Italian player in full view of hundreds of millions of people. The referee banished him from the field, France lost the match, and a single surrender to feeling forever stained the brilliant career Zidane had dedicated his life to building.

In contrast to feeling, thinking is a conscious mental process performed to solve a problem, make a decision, or gain understanding. Whereas feeling has no purpose beyond expressing itself, thinking aims beyond itself to knowledge or action. This is not to say that thinking is infallible; in fact, a good part of this book is devoted to exposing errors in thinking and showing you how to avoid them. Yet for all its shortcomings, thinking is the most reliable guide to action we humans possess. To sum up the relationship between feeling and thinking, feelings need to be tested before being trusted, and thinking is the most reasonable and reliable way to test them.

There are three broad categories of thinking: reflective, creative, and critical. The focus of this book is on critical thinking. The essence of critical thinking is evaluation. Critical thinking, therefore, may be defined as the process by which we test claims and arguments and determine which have merit and which do not. In other words, critical thinking is a search for answers, a quest. Not surprisingly, one of the most important techniques used in critical thinking is asking probing questions. Where the uncritical accept their first thoughts and other people’s statements at face value, critical thinkers challenge all ideas in this manner:

Thought
1. Professor Vile cheated me in my composition grade. He weighted some themes more heavily than others.
2. Before women entered the work force, there were fewer divorces. That shows that a woman’s place is in the home.
3. A college education isn’t worth what you pay for it. Some people never reach a salary level appreciably higher than the level they would have reached without the degree.
Question
1. Did he grade everyone on the same standard? Were the different weightings justified?
2. How do you know that this factor, and not some other one(s), is responsible for the increase in divorces?
3. Is money the only measure of the worth of an education? What about increased understanding of self and life and increased ability to cope with challenges?

Critical thinking also employs questions to analyze issues. Consider, for example, the subject of values. When it is being discussed, some people say, "Our country has lost its traditional values" and "There would be less crime, especially violent crime, if parents and teachers emphasized moral values." Critical thinking would prompt us to ask,

1. What is the relationship between values and beliefs? Between values and convictions?
2. Are all values valuable?
3. How aware is the average person of his or her values? Is it possible that many people deceive themselves about their real values?
4. Where do one’s values originate? Within the individual or outside? In thought or in feeling?
5. Does education change a person’s values? If so, is this change always for the better?
6. Should parents and teachers attempt to shape children’s values?

\section{Characteristics of Critical Thinkers}

A number of misconceptions exist about critical thinking. One is that being able to support beliefs with reasons makes one a critical thinker. Virtually everyone has reasons, however weak they may be. The test of critical thinking is whether the reasons are good and sufficient.

Another misconception is that critical thinkers never imitate others in thought or action. If that were the case, then every eccentric would be a critical thinker. Critical thinking means making sound decisions, regardless of how common or uncommon those decisions are.

It is also a misconception that critical thinking is synonymous with having a lot of right answers in one’s head. There’s nothing wrong with having right answers, of course. But critical thinking involves the process of finding answers when they are not so readily available.

And yet another misconception is that critical thinking cannot be learned, that one either has it or does not. On the contrary, critical thinking is a matter of habit. The most careless, sloppy thinker can become a critical thinker by developing the characteristics of a critical thinker. This is not to say that all people have equal thinking potential but rather that everyone can achieve dramatic improvement.

We have already noted one characteristic of critical thinkers-skill in asking appropriate questions. Another is control of one’s mental activities. John Dewey once observed that more of our time than most of us care to admit is spent "trifling with mental pictures, random recollections, pleasant but unfounded hopes, flitting, half-developed impressions."7 Good thinkers are no exception. However, they have learned better than poor thinkers how to stop that casual, semiconscious drift of images when they wish and how to fix their minds on one specific matter, examine it carefully, and form a judgment about it. They have learned, in other words, how to take charge of their thoughts, to use their minds actively as well as passively.

Here are some additional characteristics of critical thinkers, as contrasted with those of uncritical thinkers:

Critical Thinkers . . .
1. Are honest with themselves, acknowledging what they don’t know, recognizing their limitations, and being watchful of their own errors.
2. Regard problems and controversial issues as exciting challenges.
3. Strive for understanding, keep curiosity alive, remain patient with complexity, and are ready to invest time to overcome confusion.
4. Base judgments on evidence rather than personal preferences, deferring judgment whenever evidence is insufficient. They revise judgments when new evidence reveals error.
5. Are interested in other people’s ideas and so are willing to read and listen attentively, even when they tend to disagree with the other person.
6. Recognize that extreme views (whether conservative or liberal) are seldom correct, so they avoid them, practice fairmindedness, and seek a balanced view.
7. Practice restraint, controlling their feelings rather than being controlled by them, and thinking before acting.

Uncritical Thinkers . . .
1. Pretend they know more than they do, ignore their limitations, and assume their views are error-free.
2. Regard problems and controversial issues as nuisances or threats to their ego.
3. Are impatient with complexity and thus would rather remain confused than make the effort to understand.
4. Base judgments on first impressions and gut reactions. They are unconcerned about the amount or quality of evidence and cling to their views steadfastly.
5. Are preoccupied with themselves and their own opinions and so are unwilling to pay attention to others’ views. At the first sign of disagreement, they tend to think, "How can I refute this?"
6. Ignore the need for balance and give preference to views that support their established views.
7. Tend to follow their feelings and act impulsively.

As the desirable qualities suggest, critical thinking depends on mental discipline. Effective thinkers exert control over their mental life, direct their thoughts rather than being directed by them, and withhold their endorsement of any idea-even their own-until they have tested and confirmed it. John Dewey equated this mental discipline with freedom. That is, he argued that people who do not have it are not free persons but slaves to whim or circumstance:

> If a man’s actions are not guided by thoughtful conclusions, then they are guided by inconsiderate impulse, unbalanced appetite, caprice, or the circumstances of the moment. To cultivate unhindered, unreflective external activity is to foster enslavement, for it leaves the person at the mercy of appetite, sense, and circumstance.

\section{The Role of Intuition}

Intuition is commonly defined as immediate perception or comprehension of something-that is, sensing or understanding something without the use of reasoning. Some everyday experiences seem to support this definition. You may have met a stranger and instantly "known" that you would be partners for life. When a car salesman told you that the price he was quoting you was his final, rock-bottom price, your intuition may have told you he was lying. On the first day of a particular course, you may have had a strong sense that you would not do well in it.

Some important discoveries seem to have occurred instantaneously. For example, the German chemist Kekule found the solution to a difficult chemical problem intuitively. He was very tired when he slipped into a daydream. The image of a snake swallowing its tail came to him-and that provided the clue to the structure of the benzene molecule, which is a ring, rather than a chain, of atoms. The German writer Goethe had been experiencing great difficulty organizing a large mass of material for one of his works when he learned of the tragic suicide of a close friend. At that very instant, the plan for organizing his material occurred to him in detail. The English writer Samuel Taylor Coleridge (you may have read his Rime of the Ancient Mariner in high school) awoke from a dream with 200–300 lines of a new and complex poem clearly in mind.

Such examples seem to suggest that intuition is very different from reasoning and is not influenced by it. But before accepting that conclusion, consider these facts:

> Breakthrough ideas favor trained, active minds. It is unusual for someone totally untrained in a subject to make a significant new discovery about it. Thus, if Kekule had been a plumber, Goethe a bookkeeper, and Coleridge a hairdresser, they would almost certainly not have received the intuitions for which they are famous.
> Some intuitions eventually prove to be mistaken. That attractive stranger may turn out to be not your lifelong partner but a person for whom you develop a strong dislike. The car salesman’s final price may have proved to be exactly that. And instead of doing poorly in that course, you may have done well.
> It is difficult to make an overall assessment of the quality of our intuitions because we tend to forget the ones that prove mistaken in much the same way a gambler forgets his losses.

These facts have led some scholars to conclude that intuition is simply a consequence of thinking. They would say that something about the stranger appealed to you, something the salesman said or did suggested insincerity, something about the professor frightened you. In each case, they would explain, you made a quick decision-so quick, in fact, that you were unaware that you’d been thinking. In the case of the breakthrough ideas, the scholars would say that when people become engrossed in problems or issues, their unconscious minds often continue working on them long after they have turned their attention elsewhere. Thus, when an insight seems to come "out of nowhere," it is actually a delayed result of thinking.

Which view of intuitions is the correct one? Are intuitions different from and independent of thinking or not? Perhaps, for now, the most prudent answer is that sometimes they are independent and sometimes they are not; we can’t be sure when they are, and therefore it is imprudent to rely on them.

\section{Basic Activities in Critical Thinking}

The basic activities in critical thinking are investigation, interpretation, and judgment, in that order. The following chart summarizes each activity in relation to the other two.

Investigation
    Definition: Finding evidence-that is, data that will answer key questions about the issue
    Requirements: The evidence must be both relevant and sufficient.
Interpretation
    Definition: Deciding what the evidence means
    Requirements: The interpretation must be more reasonable than competing interpretations.
Judgment
    Definition: Reaching a conclusion about the issue
    Requirements: The conclusion must meet the test of logic.

As we noted previously, irresponsible thinkers first choose their conclusions and then seek out evidence to justify their choices. They fail to realize that the only conclusion worth drawing is one based on a thorough understanding of the problem or issue and its possible solutions or resolutions. Is it acceptable to speculate, guess, and form hunches and hypotheses? Absolutely. Such activities provide a helpful starting point for the thinking process. (Besides, we couldn’t avoid doing so even if we tried.) The crucial thing is not to let hunches and hypotheses manipulate our thinking and dictate our conclusion in advance.

\section{Critical Thinking and Writing}

Writing may be used for either of two broad purposes: to discover ideas or to communicate them. Most of the writing you have done in school is undoubtedly the latter kind. But the former can be very helpful, not only in sorting out ideas you’ve already produced, but also in stimulating the flow of new ideas. For some reason, the very act of writing down one idea seems to generate additional ideas.

Whenever you write to discover ideas, focus on the issue you are examining and record all your thoughts, questions, and assertions. Don’t worry about organization or correctness. If ideas come slowly, be patient. If they come suddenly, in a rush, don’t try to slow down the process and develop any one of them; simply jot them all down. (There will be time for elaboration and correction later.) Direct your mind’s effort, but be sensitive to ideas on the fringe of consciousness. Often they, too, will prove valuable.

If you have done your discovery writing well and have thought critically about the ideas you have produced, the task of writing to communicate will be easier and more enjoyable. You will have many more ideas-carefully evaluated ones-to develop and organize.

\section{Critical Thinking and Discussion}

At its best, discussion deepens understanding and promotes problem solving and decision making. At its worst, it frays nerves, creates animosity, and leaves important issues unresolved. Unfortunately, the most prominent models for discussion in contemporary culture-radio and TV talk shows-often produce the latter effects.

Many hosts demand that their guests answer complex questions with simple "yes" or "no" answers. If the guests respond that way, they are attacked for oversimplifying. If, instead, they try to offer a balanced answer, the host shouts, "You’re not answering the question," and proceeds to answer it himself. Guests who agree with the host are treated warmly; others are dismissed as ignorant or dishonest. Often as not, when two guests are debating, each takes a turn interrupting while the other shouts, "Let me finish." Neither shows any desire to learn from the other. Typically, as the show draws to a close, the host thanks the participants for a "vigorous debate" and promises the audience more of the same next time.

Here are some simple guidelines for ensuring that the discussions you engage in-in the classroom, on the job, or at home-are more civil, meaningful, and productive than what you see on TV. By following these guidelines, you will set a good example for the people around you.

*Whenever possible, prepare in advance.* Not every discussion can be prepared for in advance, but many can. An agenda is usually circulated several days before a business or committee meeting. In college courses, the assignment schedule provides a reliable indication of what will be discussed in class on a given day. Use this information to prepare: Begin by reflecting on what you already know about the topic. Then decide how you can expand your knowledge and devote some time to doing so. (Fifteen or twenty minutes of focused searching in the library or on the Internet can produce a significant amount of information on almost any subject.) Try to anticipate the different points of view that might be expressed in the discussion and consider the relative merits of each. Keep your conclusions tentative at this point, so that you will be open to the facts and interpretations others will present.

*Set reasonable expectations.* Have you ever left a discussion disappointed that others hadn’t abandoned their views and embraced yours? Have you ever felt offended when someone disagreed with you or asked you what evidence you had to support your opinion? If the answer to either question is yes, you probably expect too much of others. People seldom change their minds easily or quickly, particularly in the case of long-held convictions. And when they encounter ideas that differ from their own, they naturally want to know what evidence supports those ideas. Expect to have your ideas questioned, and be cheerful and gracious in responding.

*Leave egotism and personal agendas at the door.* To be productive, discussion requires an atmosphere of mutual respect and civility. Egotism produces disrespectful attitudes toward others-notably, "I’m more important than other people," "My ideas are better than anyone else’s," and "Rules don’t apply to me." Personal agendas, such as dislike for another participant or excessive zeal for a point of view, can lead to personal attacks and unwillingness to listen to others’ views.

*Contribute but don’t dominate.* If you are the kind of person who loves to talk and has a lot to say, you probably contribute more to discussions than other participants. On the other hand, if you are more reserved, you may seldom say anything. There is nothing wrong with being either kind of person. However, discussions tend to be most productive when everyone contributes ideas. For this to happen, loquacious people need to exercise a little restraint, and more reserved people need to accept responsibility for sharing their thoughts.

*Avoid distracting speech mannerisms.* Such mannerisms include starting one sentence and then abruptly switching to another; mumbling or slurring your words; and punctuating every phrase or clause with audible pauses ("um," "ah,") or meaningless expressions ("like," "you know," "man"). These annoying mannerisms distract people from your message. To overcome them, listen to yourself when you speak. Even better, tape your conversations with friends and family (with their permission), then play the tape back and listen to yourself. Whenever you are engaged in a discussion, aim for clarity, directness, and economy of expression.

*Listen actively.* When the participants don’t listen to one another, discussion becomes little more than serial monologue-each person taking a turn at speaking while the rest ignore what is being said. This can happen quite unintentionally because the mind can process ideas faster than the fastest speaker can deliver them. Your mind may get tired of waiting and wander about aimlessly like a dog off its leash. In such cases, instead of listening to the speaker’s words, you may think about her clothing or hairstyle or look outside the window and observe what is happening there. Even when you make a serious effort to listen, it is easy to lose focus. If the speaker’s words trigger an unrelated memory, you may slip away to that earlier time and place. If the speaker says something you disagree with, you may begin framing a reply. The best way to maintain your attention is to be alert for such distractions and to resist them. Strive to enter the speaker’s frame of mind, understand what is said, and connect it with what was said previously. Whenever you realize your mind is wandering, drag it back to the task.

*Judge ideas responsibly.* Ideas range in quality from profound to ridiculous, helpful to harmful, ennobling to degrading. It is therefore appropriate to pass judgment on them. However, fairness demands that you base your judgment on thoughtful consideration of the overall strengths and weaknesses of the ideas, not on initial impressions or feelings. Be especially careful with ideas that are unfamiliar or different from your own because those are the ones you will be most inclined to deny a fair hearing.

*Resist the urge to shout or interrupt.* No doubt you understand that shouting and interrupting are rude and disrespectful behaviors, but do you realize that in many cases they are also a sign of intellectual insecurity? It’s true. If you really believe your ideas are sound, you will have no need to raise your voice or to silence the other person. Even if the other person resorts to such behavior, the best way to demonstrate confidence and character is by refusing to reciprocate. Make it your rule to disagree without being disagreeable.

\section{Avoiding Plagiarism}

Once ideas are put into words and published, they become intellectual property, and the author has the same rights over them as he or she has over a material possession such as a house or a car. The only real difference is that intellectual property is purchased with mental effort rather than money. Anyone who has ever wracked his or her brain trying to solve a problem or trying to put an idea into clear and meaningful words can appreciate how difficult mental effort can be.

Plagiarism is passing off other people’s ideas or words as one’s own. It is doubly offensive in that it both steals and deceives. In the academic world, plagiarism is considered an ethical violation and is punished by a failing grade for a paper or a course or even by dismissal from the institution. Outside the academy, it is a crime that can be prosecuted if the person to whom the ideas and words belong wishes to bring charges. Either way, the offender suffers dishonor and disgrace, as the following examples illustrate:

* When a university in South Africa learned that professor Marks Chabel had plagiarized most of his doctoral dissertation from Kimberly Lanegran of the University of Florida, the university fired Chabel. Moreover, the university that had awarded him his Ph.D. revoked it.
* When U.S.Senator Joseph Biden was seeking the 1988 Democratic presidential nomination, it was revealed that he had plagiarized passages from speeches by British politician Neil Kinnock and by Robert Kennedy. It was also learned that, while in law school, he had plagiarized a number of pages from a legal article. The ensuing scandal led Biden to withdraw his candidacy and has continued to stain his reputation.
*  The reputation of historian Stephen Ambrose was tarnished by allegations that over the years he plagiarized the work of several authors. Doris Kearns Goodwin, historian and advisor to President Lyndon Johnson, suffered a similar embarrassment when she was discovered to have plagiarized from more than one source in one of her books.
* When James A. Mackay, a Scottish historian, published a biography of Alexander Graham Bell in 1998, Robert Bruce presented evidence that the book was largely plagiarized from his 1973 biography, which had won a Pulitzer Prize. Mackay was forced to withdraw his book from the market. (Incredibly, he did not learn from the experience because he then published a biography of John Paul Jones, which was plagiarized from a 1942 book by Samuel Eliot Morison.)
* When New York Times reporter Jason Blair was discovered to have plagiarized stories from other reporters and fabricated quotations and details in his stories, he resigned his position in disgrace. Soon afterward, the two senior editors who had been his closest mentors also resigned, reportedly because of their irresponsible handling of Blair’s reportage and the subsequent scandal.

Some cases of plagiarism are attributable to intentional dishonesty, others to carelessness. But many, perhaps most, are due to misunderstanding.  The instructions "Base your paper on research rather than on your own unfounded opinions" and "Don’t present other people’s ideas as your own" seem contradictory and may confuse students, especially if no clarification is offered. Fortunately, there is a way to honor both instructions and, in the process, to avoid plagiarism.

Step 1: When you are researching a topic, keep your sources’ ideas separate from your own. Begin by keeping a record of each source of information you consult. For an Internet source, record the Web site address, the author and title of the item, and the date you visited the site.  For a book, record the author, title, place of publication, publisher, and date of publication. For a magazine or journal article, record the author, title, the name of the publication, and its date of issue. For a TV or radio broadcast, record the program title, station, and date of transmission.

Step 2: As you read each source, note the ideas you want to refer to in your writing. If the author’s words are unusually clear and concise, copy them exactly and put quotation marks around them. Otherwise, paraphrase- that is, restate the author’s ideas in your own words. Write down the number(s) of the page(s) on which the author’s passage appears.

If the author’s idea triggers a response in your mind-such as a question, a connection between this idea and something else you’ve read, or an experience of your own that supports or challenges what the author says-write it down and put brackets (not parentheses) around it so that you will be able to identify it as your own when you review your notes.  Here is a sample research record illustrating these two steps:

> Adler, Mortimer J. The Great Ideas: A Lexicon of Western Thought (New York: Macmillan, 1992) Says that throughout the ages, from ancient Greece, philosophers have argued about whether various ideas are true. Says it’s remarkable that most renowned thinkers have agreed about what truth is-"a correspondence between thought and reality." 867 Also says that Freud saw this as the scientific view of truth. Quotes Freud: "This correspondence with the real external world we call truth. It is the aim of scientific work, even when the practical value of that work does not interest us." 869 [I say true statements fit the facts; false statements do not.]

Whenever you look back on this record, even a year from now, you will be able to tell at a glance which ideas and words are the author’s and which are yours. The first three sentences are, with the exception of the directly quoted part, paraphrases of the author’s ideas. Next is a direct quotation. The final sentence, in brackets, is your own idea.

Step 3: When you compose your paper, work borrowed ideas and words into your own writing by judicious use of quoting and paraphrasing.  In addition, give credit to the various authors. Your goal here is to eliminate all doubt about which ideas and words belong to whom. In formal presentations, this crediting is done in footnotes; in informal ones, it is done simply by mentioning the author’s name.

Here is an example of how the material from Mortimer Adler might be worked into a composition. (Note the form that is used for the footnote.) The second paragraph illustrates how your own idea might be expanded:

> Mortimer J. Adler explains that throughout the ages, from the time of the ancient Greeks, philosophers have argued about whether various ideas are true. But to Adler the remarkable thing is that, even as they argued, most renowned thinkers have agreed about what truth is. They saw it as "a correspondence between thought and reality." Adler points out that Sigmund Freud believed this was also the scientific view of truth. He quotes Freud as follows: "This correspondence with the real external world we call truth. It is the aim of scientific work, even when the practical value of that work does not interest us."*

> This correspondence view of truth is consistent with the commonsense rule that a statement is true if it fits the facts and false if it does not. For example, the statement "The twin towers of New York’s World Trade Center were destroyed on September 11, 2002," is false because they were destroyed the previous year. I may sincerely believe that it is true, but my believing in no way affects the truth of the matter. In much the same way, if an innocent man is convicted of a crime, neither the court’s decision nor the world’s acceptance of it will make him any less innocent. We may be free to think what we wish, but our thinking can’t alter reality

> *Mortimer J. Adler, The Great Ideas: A Lexicon of Western Thought (New York: Macmillan, 1992), pp. 867, 869.

\section{Applications}

1. Think back on your previous schooling. How closely has your experience matched Arthur’s? Explain.
2. Reflect on your powers of concentration. Do you find it difficult to ponder important matters? Are you able to prevent the casual, semiconscious drift of images from interrupting your thoughts? Do you have less control in some situations than in others? Explain.
3. Rate yourself on each of the eight characteristics of good critical thinkers that are listed on pp. 24–26. Which are you strongest in? Which weakest? If your behavior varies from situation to situation, try to determine what kinds of issues or circumstances bring out your best and worst mental qualities.
4. Consider how you approach problems and issues. Is there any pattern to the way you think about a problem or an issue? Does an image come to mind first? Or perhaps a word? What comes next? And what after that? If you can’t answer these questions completely, do this exercise: Flip half a dozen pages ahead in this book, pick a sentence at random, read it, and note how your mind deals with it. (Such thinking about your thinking may be a little awkward at first. If it is, try the exercise two or three times.)
5. Read each of the following statements carefully. Then decide what question(s), if any, a good critical thinker would find it appropriate to ask.
a. Television news sensationalizes its treatment of war because it gives us pictures only of injury, death, and destruction.
b. My parents were too strict-they wouldn’t let me date until I was sixteen.
c. It’s clear to me that Ralph doesn’t care for me-he never speaks when we pass in the hall.
d. From a commercial for a news network: "The news is changing every minute of the day, so you constantly need updating to keep you informed."
e. The statement of an Alabama public elementary school teacher who had students recite the Lord’s Prayer and say grace before meals: "I feel part of my job as a teacher is to instill values children need to have a good life."

\section{A Difference of Opinion}

The following passage summarizes an important difference of opinion. After reading the statement, use the library and/or the Internet and find what knowledgeable people have said about the issue. Be sure to cover the entire range of views. Then assess the strengths and weaknesses of each. If you conclude that one view is entirely correct and the others are mistaken, explain how you reached that conclusion. If, as is more likely, you find that one view is more insightful than the others but that they all make some valid points, construct a view of your own that combines the insights from all views and explain why that view is the most reasonable of all. Present your response in a composition or an oral report, as your instructor specifies.

> What response should the United States make to the problem of illegal immigration? As violence on the southern U.S. border increases and illegal entry continues, many Americans are becoming impatient with the federal government’s failure to solve the border problem. The state of Arizona has already taken action to apprehend illegals but has been criticized for interfering in matters under federal jurisdiction. Is Arizona’s approach the most reasonable one? If not, what approach would be? 

Begin your analysis by conducting a Google search using the terms "Arizona illegal immigrants" and "border security issues."

\chapter{What Is Truth?}

For hundreds of years, philosophers battled over whether "truth" exists. The argument usually concerned Truth with a capital T, a kind of complete record of whatever was, is, or will be, error-proof, beyond doubt and dispute, a final test of the rightness or wrongness of people’s ideas and theories.

Those who accepted the existence of this Truth believed it was a spiritual reality, not a physical one. That is, it was not a celestial ledger or file drawer-yet it was beyond time and space. It was considered an understanding among the gods, or an idea in the mind of God, or simply the sum total of Reality. Could humans ever come to know Truth? Some said, no, never. Others said, yes but only in the afterlife. Still others said that the wisest and best of humans could catch glimpses of it and that the rest of humanity could learn about it through these special ones.

Those who rejected this notion of an awesome, all-embracing Truth argued that it was an empty notion. How could all reality be summed up that way? More important, what possible evidence could be offered in support of its existence? Many who reasoned this way dismissed the idea of Truth as wishful thinking, a kind of philosophical security blanket. A few went further and denied even the existence of truths (no capital).

Our age has inherited the whole argument. The focus, however, has changed. It seldom concerns Truth anymore. Even if Truth does exist, it’s of little help to us in our world and our lives because it is beyond human understanding. Even many people of strong and rather conservative religious views no longer consider the question of Truth important to the understanding or practice of their faith.

Still, the question of truth, or even truths, remains, and the position we take toward this question does have an important bearing on how we conduct our thinking and acting. Unfortunately, there is a good deal of murkiness and confusion about the concept. The rest of this chapter will attempt to shed light on it.

It’s fashionable today to believe that truth is relative and subjective.  "Everyone creates his or her own truth," the saying goes, "and what is true for you may not be true for me." The meaning of this statement goes far beyond "It’s a free country and I can believe what I want." The claim means that whatever a person thinks is true because he or she thinks it is. Not surprisingly, to challenge another person’s view on an issue is considered bad form. "That’s my truth you’re talking about, Buster. Show a little respect."

The implications of this notion are quite staggering, yet for some reason few people acknowledge them, and fewer still are interested in testing their reasonableness. One implication is that everyone is right and no one is wrong. In fact, no one can be wrong. (What an argument this would make against objective tests-true/false, multiple choice, and so on: "My answers can’t be wrong, professor. They’re my truth!") Another is that everyone’s perception and memory work flawlessly, with never a blunder, glitch, or gaffe. And another is that no one adopts other people’s "truths." The idea of creating truth rules out borrowing-if truth is intensely personal, each person’s truth must be unique. Let’s examine all these ideas more closely.

\section{Where Does It All Begin?}

The idea of creating our own truth without outside influence or assistance may sound reasonable if we focus only on our adulthood. The moment we consider our childhood, however, the idea becomes suspect, because in childhood we were all dependent in every sense: physically, emotionally, and intellectually. What we knew and believed about everything was what others told us. We asked questions-"Why, Mommy?" "Why, Daddy?" Our parents answered them. We accepted those answers and made them the foundation of our belief system, no matter how elaborate it would become in adulthood.

Relativists could, of course, claim that we leave all those early influences behind when we reach adulthood, but that denies the most fundamental principles of psychology. Here is how one writer explained the continuing influence of childhood experience:

> We are told about the world before we see it. We imagine most things before we experience them. And those preconceptions, unless education has made us acutely aware, govern deeply the whole process of perception.  They mark out certain objects as familiar or strange, emphasizing the difference, so that the slightly familiar is seen as very familiar, and the somewhat strange as sharply alien. They are aroused by small signs, which may vary from a true index to a vague analogy. Aroused, they flood fresh vision with older images, and project into the world what has been resurrected in memory.

You have heard the old saying seeing is believing. The reverse-believing is seeing-is equally correct. To a greater or lesser extent, what we regard as our unique perspective bears the imprint of other people’s ideas and beliefs.

\section{Imperfect Perception}

Is perception flawless? Hardly. For one thing, it is influenced by our desires, interests, and expectations: "From the outset perception is selective and tends to simplify the world around us. Memory continues and hastens the process." For another, even within its limited focus, perception is often flawed. A college student who is positive that the textbook contains a certain statement answers an exam question with perfect confidence. Yet when the student gets the corrected test back and finds the question marked wrong, then hurriedly flips open the book and examines the passage again, he or she may find it says something else entirely.

Moviegoers in the 1930s and 1940s were thrilled as Tarzan uttered his famous yell and swung through the treetops to catch the villain. Tell them that Tarzan never made that yell and they’ll say, "False, we heard it with our own ears." And yet it’s not false. According to one of the men who first played the role of Tarzan, Buster Crabbe, that yell was dubbed into the films in the studio. It was a blend of three voices-a soprano’s, a baritone’s, and a hog caller’s.

At least a dozen times every weekend from September to January, the imperfection of human observation is underlined by that marvel of technology, the instant replay. Is there a football fan anywhere who doesn’t occasionally scream, "Bad call!" only to be proved wrong a moment later? We can be sure enough to bet a week’s wages that the pass receiver’s feet came down inbounds or that the running back’s knee hit the ground before the ball came loose. And then the replay shows us how erroneous our initial perception was.

The vagaries of perception have long been noted by those who deal with human testimony-notably, trial lawyers, police officers, and psychologists.  It is well established that a number of factors can make us see and hear inaccurately. Darkness, cloudy conditions, or distance from what we are witnessing may obscure our vision. We may be distracted at a crucial moment. If we are tired or in the grip of powerful emotions such as fear or anger, our normal perceptiveness may be significantly diminished.  Also, perception may be intermingled with interpretation-the expectation that an event will unfold in a certain way may color our perception of the way the event actually unfolds. Loyalty and affection toward the people or things involved may distort our vision as well. If someone we dislike speaks in a loud voice and is animated, we may regard that person as showing off to get attention. But if a friend behaves in the same way, we may regard him or her as vivacious and extroverted.

\section{Imperfect Memory}

Even when our perception is initially flawless, our memory often distorts the data. We forget details, and when later attempting to recall what happened we resort to imagination to fill in the blanks. Though we may at first be aware that such a process of reconstruction is occurring, this awareness soon fades, and we come to believe we are remembering the original perception. As psychologist William James explained,

> The most frequent source of false memory is the accounts we give to others of our experiences. Such acts we almost always make more simple and more interesting than the truth. We quote what we should have said or done rather than what we really said or did; and in the first telling we may be fully aware of the distinction, but [before] long the fiction expels the reality from memory and [replaces it]. We think of what we wish had happened, of possible [interpretations] of acts, and soon we are unable to distinguish between things that actually happened and our own thoughts about what might have occurred. Our wishes, hopes, and sometimes fears are the controlling factor.

As if this weren’t enough, memory is vulnerable to contamination from outside the mind. Memory expert Elizabeth Loftus showed children a one-minute film and then asked, "Did you see a bear?" or "Did you see a boat?" They remembered seeing them, even though no bears or boats were in the film. She also showed adults a film of an auto accident and then asked them about it. By using the word "smash" instead of "hit," she was able to change the viewers’ estimate of the cars’ speed and to create a memory of broken glass where there was none. In another experiment, Loftus asked the parents of college students to describe some events from their sons’ and daughters’ childhoods. Then she talked with each student about those events but added a fake event or two. With only slight coaxing, the students "remembered" the fake events, were able to elaborate on the details, and in some cases refused to believe they were fake even when Loftus explained what she had done.

\section{Deficient Information}

The quality of a belief depends to a considerable extent on the quality of the information that backs it up. Because it’s a big world and reality has many faces, it’s easy for us to be misinformed. How many drivers take the wrong turn because of faulty directions? How many people get on the wrong bus or train? How many car owners put too much or too little air in their tires on the advice of some service station attendant? And, if misinformation is common enough in such relatively simple matters, how much more common is it in complex matters like law and medicine and government and religion?

It’s possible, of course, to devote a lifetime of study to a particular field. But not even those who make that kind of commitment can know everything about their subject. Things keep happening too fast. They occur whether we’re watching or not. There’s no way to turn them off when we take a coffee break or go to the bathroom. The college student who hasn’t been home in three months may be able to picture the neighbor’s elm tree vividly, yet it may have been cut down two months ago. The soldier may have total recall of his hometown-every sight and sound and smell-and return home to find half of Main Street sacrificed to urban renewal, the old high school hangout closed, and a new car in his best friend’s driveway.

\section{Even the Wisest Can Err}

So far, we’ve established that people can be mistaken in what they perceive and remember and that the information they receive can be faulty or incomplete. But these matters concern individuals. What of group judgment-the carefully analyzed observations of the best thinkers, the wisest men and women of the time? Is that record better? Happily, it is.  But it, too, leaves a lot to be desired.

All too often, what is taken as truth one day by the most respected minds is proved erroneous the next. You undoubtedly know of some examples. In the early seventeenth century, when Galileo suggested that the sun is the center of our solar system, he was charged with heresy, imprisoned, and pressured to renounce his error. The "truth" of that time, accepted by every scientist worthy of the name, was that the earth was the center of the solar system.

Here are some other examples you may not have heard about in which the "truth" turned out not to be true:

* For a long time surgeons used talc on the rubber gloves they wore while performing surgery. Then they discovered it could be poisonous.  So they switched to starch, only to find that it, too, could have a toxic effect on surgical patients.
* Film authorities were certain they were familiar with all the films the late Charlie Chaplin ever made. Then, in 1982, a previously unknown film was discovered in a British screen archive vault.
* For hundreds of years historians believed that although the people of Pompeii had been trapped by the eruption of Mount Vesuvius in A.D. 79, the people of neighboring Herculaneum had escaped. Then the discovery of eighty bodies (and the hint of hundreds more) under the volcanic ash revealed that many from Herculaneum had also been trapped.
* Your grandparents probably learned that there are eight planets in our solar system. Since Pluto was discovered in 1930, your parents and you learned there are nine. Then Joseph L. Brady of the University of California suggested there might be ten.   But more recently Pluto was removed from the list.
* After morphine was used by doctors for some years as a painkiller, it was found to be addictive. The search began for a nonaddictive substitute. What was found to take its place? Heroin!

\section{Truth Is Discovered, Not Created}

Let’s review what our evaluation has revealed. First, our ideas and beliefs are unavoidably influenced by other people’s, particularly in childhood.  Second, perception and memory are imperfect. Third, our information can be inaccurate or incomplete. Add to this the fact, noted in Chapter 2, that some people’s thinking skills are woefully meager and/or ineffectively used, and the idea that "everyone creates his or her own truth" becomes laughable. We do create something, all right, but it is not truth. It is beliefs, ideas that we accept as true but that could easily be false.

What, then, is the most reasonable view of truth? The truth about something is what is so about it-the facts in their exact arrangement and proportions. Our beliefs and assertions are true when they correspond to that reality and false when they do not.

Did time run out before the basketball player got the shot off? How does gravity work? Who stole your hubcaps? Are there time/space limits to the universe? Who started the argument between you and your neighbor last weekend? Have you been working up to your potential in this course? To look for the truth in such matters is to look for the answer that fits the facts, the correct answer.

Truth is apprehended by discovery, a process that favors the curious and the diligent. Truth does not depend on our acknowledgment of it, nor is it in any way altered by our ignorance or transformed by our wishful thinking. King Tut’s tomb did not spring into existence when archaeologists dug it up; it was there waiting to be discovered. Art forgeries are not genuine when people are fooled and then fake when the deception is revealed. Cigarette smoking is not rendered harmless to our health because we would prefer it to be so.

Much of the confusion about truth arises from complex situations in which the truth is difficult to ascertain or express. Consider a question like Are there really UFOs that are piloted by extraterrestrial beings? Although the question is often hotly debated and people make assertions that purport to express the truth, there is not yet sufficient evidence to say we know the truth about UFOs. However, that doesn’t mean there is no truth about them or that people who affirm their existence and people who deny it are equally correct. It means that whatever the truth is, we do not yet possess it.

Similar difficulty arises from many psychological and philosophical questions-for example: Why are some people heterosexual and others homosexual? Is the cause of criminality genetic or environmental or a combination of the two? Are humans inherently violent? Is there an afterlife?  What constitutes success? The answers to these questions, and to many of the issues you will encounter in the applications in this book, will often be incomplete or tentative. Yet that fact should not shake your conviction that there are truths to be discovered.

When planes crashed into the twin towers of the World Trade Center and the Pentagon on September 11, 2001, killing several thousand people, the event was officially classified as a terrorist attack. But before long, a very different theory was advanced-that individuals in the highest levels of the U.S. government had planned and executed the crashes to provide an excuse for attacking Iraq. This conspiracy theory gained a number of well-known supporters, including movie and television stars and at least one member of Congress, and was disseminated around the world. In France, for example, a book supporting the theory became a best-seller.  The issue became the subject of international debate-in some quarters, people are still divided in their views. But to my knowledge, not a single individual, in this country or abroad, took the position that both views are correct-that is, that each side is entitled to its own truth. If anyone had, he or she would have been attacked by both camps for talking nonsense and trivializing an important issue. When it comes to significant events like 9/11, people want to know the truth, what really happened.

Having the right frame of mind can make your pursuit of the truth less burdensome and give it the sense of adventure that the great thinkers in history experienced. A good way to begin is to keep the following thought in mind: "I know I have limitations and can easily be mistaken. And surely I’ll never find all the answers I’d like to. But I can observe a little more accurately, weigh things a little more thoroughly, and make up my mind a little more carefully. If I do so, I’ll be a little closer to the truth."

That’s far different from saying, "Everyone makes his or her own truth" or "It all depends on how you look at it." And it is much more reasonable.

\section{Understanding Cause and Effect  }

Some of the most difficult challenges in discovering truth occur in determining cause-and-effect relationships. Unfortunately, mistakes are common in such matters. One mistake is to see cause-and-effect relationships where there are none. Another is to see only the simple and obvious cause-and-effect relationships and miss the complex or subtle ones. A third is to believe that causation is relevant only to material forces and is unrelated to human affairs. To avoid such confusion, four facts must be understood:

1. One event can precede another without causing it. Some people believe that when one event precedes another, it must be the cause of the other. Most superstition is rooted in this notion. For example, breaking a mirror, crossing paths with a black cat, or walking under a ladder is believed to cause misfortune. You don’t have to be superstitious to make this mistake. You may believe that your professor gave an unannounced quiz today because students were inattentive the day before yesterday, whereas he may have planned it at the beginning of the semester. Or you may believe the stock market fell because a new president took office, when other factors might have prompted the decline. 

The problem with believing that preceding events necessarily cause subsequent events is that such thinking overlooks the possibility of coincidence.  This possibility is the basis of the principle that "correlation does not prove causation." In order to establish a cause-and-effect relationship, it is necessary to rule out coincidence, or at least to make a persuasive case against it.

2. Not all causation involves force or necessity. The term causation is commonly associated with a physical action affecting a material reality, such as, a lightning bolt striking a house and the house catching fire and burning. Or a flowerpot being accidentally dropped out a window and then falling to the ground and breaking. Or a car speeding, failing to negotiate a curve, careening off the highway, and crashing into a tree. In such cases a scientific principle or law applies (combustion, gravity, inertia), and the effect is inevitable or at least highly predictable. 

That type of causation is valid, but it would be a mistake to think of it as the only type. Causation also occurs in the nonmaterial realities we call human affairs-more specifically, in the processes of emotion and thought. That type of causation has little, if anything, to do with scientific principles or laws, is almost never inevitable, and is often difficult to predict.  If we are to avoid oversimplification, we need to define causation in a way that covers both the scientific realm and the realm of human affairs.  Here is a footnote for this: As its first definition of cause, the Oxford English Dictionary gives "that which produces an effect; that which gives rise to any action, phenomenon, or condition." The distinction between "produces" and "gives rise to" is what we are referring to here. We will therefore define causation as the phenomenon of one thing influencing the occurrence of another. The influence may be major or minor, direct or indirect, proximate or remote in time or space. It may also be irresistible, as in the examples of combustion, gravity, and inertia mentioned previously; or resistible, as in following parental teaching or the example of one’s peers. In the latter case, and in other matters involving ideas, the influence (cause) does not force the effect to occur but instead invites, encourages, or inspires it. Consider these examples:

* The idea that intelligence is genetically determined led early twentiethcentury educators to conclude that thinking cannot be taught, and thus to emphasize rote learning and expand vocational curriculums.
* The idea that people are naturally good, and therefore not personally responsible for their bad deeds, has shifted blame to parents, teachers, and society, and caused judges to treat criminals more leniently.
* The idea that one race or ethnic group is superior to another has led to military campaigns against neighboring countries, discriminatory laws, slavery, and genocide.
* The idea that "no one over thirty can be trusted," which was popular in the United States during the 1960s and 1970s, led many young people to scorn both the advice of their parents and teachers and the accumulated wisdom of the past.
* The idea that feelings are a reliable guide to behavior has led many people to set aside restraint and follow their impulses. This change has arguably led to an increase in incivility, road rage, and spouse abuse, among other social problems.
* The idea that self-esteem is prerequisite to success changed the traditional idea of self-improvement, inspired hundreds of books focused on selfacceptance, and led educators to more indulgent views of homework, grading, and discipline.

In each of these examples, one idea influenced the occurrence of an action or belief and, in that sense, caused it. Columnist George Will no doubt had this view of causation in mind when he encountered the claim that "no one has ever dropped dead from viewing ‘Natural Born Killers,’ or listening to gangster rap records." Will responded, "No one ever dropped dead reading ‘Der Sturmer,’ the Nazi anti-Semitic newspaper, but the culture it served caused six million Jews to drop dead."

3. There is a wild card in human affairs-free will. So far we have noted that causation occurs through force or necessity in material events, but through influence in nonmaterial events-that is, in human affairs. Also, that in human affairs, effects are to some extent predictable but much less so than in material events. Now we need to consider why they are less predictable. The answer is because people possess free will-that is, the capacity to respond in ways that oppose even the strongest influences. Free will is itself a causative factor, and one that can trump all others. This explains why some people who grow up in the worst of circumstances-for example, in dysfunctional, abusive families or in crime-ridden neighborhoods in which the main sources of income are drug dealing and prostitution-resist all the negative influences and become decent, hardworking, and law-abiding. (It can also explain why some people who are more fortunate economically and socially fall short of those ideals.)

It has been rightly said that people can seldom choose the circumstances life places them in, but they can always choose their responses to those circumstances because they possess free will. In any investigation of causes and effects in human affairs, the factor of free will must be considered. However, possessing free will is no guarantee that we will apply it. In fact, one factor makes such application difficult. That factor is habit.

Habit inclines smokers to continue smoking, liars to continue lying, selfish people to go on being selfish, and countless people to unthinkingly embrace the latest fashion. When leading designers say "hemlines should be raised," hordes of women comply. When oversized beltless denim jeans are in vogue, hordes of young men waddle down the street, the tops of their pants at the middle of their hips and the crotches of their pants touching their knees. When iconic athletes shave their heads, legions of fans shave theirs. Resisting the force of habit is always possible but never easy.

The most difficult habits to break are those that accrue incrementally over time. Consider the acceptance of increasing violence and sex on TV and in films. In the 1950s, not much violence and sex were shown onscreen, and what was shown was tame. Then viewers were given glimpses of blood and gore and brief peeks at naked flesh. Year by year, the number of such scenes increased and the camera drew in a little closer and lingered a little longer over them. Over time, one thematic taboo after another was broken. Eventually violence and sexuality were joined, and themes of rape, child molestation, and even cannibalism were introduced. More recently, the industry crafted a new vehicle for assaulting the senses-the forensics program, which depicts rape-murders as they happen, then presents every gory detail of the autopsies in extreme close-up, accompanied by frequent, graphic flashbacks to refresh in viewers’ minds the shocking details of the crimes.

At first the violent and sexual content provoked protests. In time, however, as sensational images became familiar, people formed the habit of accepting them, and the protests diminished. (In time the habit grew so strong that anyone who objected to graphic sex and violence was considered odd.) What happened in this case was not that people lost their freedom or ability to protest, but instead that habit took away their inclination to protest.

4. Causation is often complex. When a small pebble is dropped into a serene pool of water, it causes ripples in every direction, and those ripples can affect even distant waters. NASA researchers have found a similar process at work in the atmosphere: tiny particles in the air called aerosols can have a rippling effect on the climate thousands of miles away from their source region.

Effects in human affairs can also be complex. In an effort to cut costs, the owner of a chemical plant may dispose of chemicals in a nearby stream that flows into a river. This action may result in effects he did not intend, including the pollution of the river, the killing of fish, and even the contracting of cancer by people living far from his plant. Those effects will be no less real because he did not intend them.

A woman in the early stages of influenza, unaware that she is ill, may sneeze while on a crowded airplane and infect dozens of her fellow passengers. As a result, they may lose time at work; some may have to be hospitalized; those with compromised immune systems could conceivably die. Given her lack of knowledge of her condition, no reasonable person would consider her culpable (morally responsible) for the effects of her sneeze, but there would still be no doubt that she caused them.

A car is driving on the interstate at night. In rapid succession, a deer jumps out and, the driver slams on his brakes but still hits and kills the deer, the car traveling closely behind slams into his car, and five other cars do likewise, each crashing into the car in front. As a result of this chain reaction, the drivers and passengers suffer a variety of injuries- minor in the case of those wearing seat belts, major in others. The task of identifying the causative factors requires careful attention to the details. The initial cause was the deer’s crossing the road at an unfortunate time, but that is not the only cause. The first driver caused the deer’s demise. Each of the other drivers caused the damage to the front end of his or her car and back end of the car in front.* And the passengers who did not fasten their seat belts caused their injuries to be more severe than those of other drivers and passengers.

These examples contain a valuable lesson about the need for care in investigating causes and effects. But this lesson will be even clearer if we examine a case in the way investigation usually proceeds-backward in time from the latest effect to the earliest causative factor; that is, to the "root" cause.

For example, it has been clear for some time that the number of people of Middle Eastern origin living in Europe has increased so dramatically that before long, according to some observers, Europe might well be called "Eurabia." What caused this change? Analysts found that for decades European companies, with their governments’ blessing, have been inviting foreigners to work in their countries, and these workers brought their families, formed their own enclaves, built their own mosques and churches, and "planted" their own ethnic cultures. The next question is what caused the governments to approve this influx of workers? The answer is that the native population of European countries had declined to a point near or below "replacement level" and there were too few native-born workers to fill the available jobs and thus fund older people’s pensions and health care services.

What caused the population decline? The availability of effective birth control techniques in the 1960s and 1970s and the choice of more and more families to employ those techniques. What caused so many families to limit the number of their children? One factor was the century-long population movement from rural areas to cities, where children are an economic burden rather than an asset. Others were the growing emphasis on self-fulfillment and the corresponding tendency to regard child rearing as self-stifling.

As even this brief analysis of causes and effects suggests, facile responses to complex issues-in this case, "Middle Easterners are trying to take over Europe" or "The Crusades are here again, in reverse"-are not only unhelpful but unfair. The following cautions will help you avoid oversimplification in your analyses:

* Remember that events seldom, if ever, "just happen." They occur as the result of specific influences, and these influences may be major or minor, direct or indirect, proximate or remote in time or space; also irresistible (forced or necessary) or resistible (invited, encouraged, or inspired).

* Remember that free will is a powerful causative factor in human affairs, and it is often intertwined with other causes. In the case of the changes in European society, the movement of people from farm to city and the use of birth control were individual choices, but the greater availability of jobs in the cities (an economic reality) and birth control technology (a scientific development) were not. 

* Be aware that in a chain of events, an effect often becomes a cause. For example, the decline in population in Europe caused the importation of foreign workers, which in turn caused a change in the ratio of native-born to foreign citizens, which may in time alter the continent’s dominant values and attitudes.

* Be aware that, in dealing with human affairs, outcomes can be unpredictable. Therefore, in determining causes, you may have to settle for probability rather than certainty (as you would in matters that lend themselves to scientific measurement). In other words, you might conclude that something is more likely than not or, when the probability is very high, substantially more likely to be the cause. Either of these conclusions has significantly more force than mere possibility, but it falls short of certainty. The difference is roughly analogous to the difference in legal standards of judgment: in civil cases, the standard is "a preponderance of the evidence" or "clear and convincing evidence," whereas in criminal cases it is the more demanding standard of "beyond a reasonable doubt."

In searching for truth, when you encounter possible cause-and-effect relationships, keep these cautions in mind.

\section{Applications}

1. Think of a recent situation in which someone referred inappropriately to "my truth." Write two or three paragraphs, in your own words, explaining to that person what you learned in this chapter.

2. A central question in sociology is How does society evolve? Three wellknown individuals gave very different answers. Auguste Comte (1798–1857) suggested that it involved three stages: religious, metaphysical, and scientific. Herbert Spencer (1820–1903) claimed that it followed Darwinian "natural selection," in which only the fittest survive. Karl Marx (1818–1883) argued that it occurred through class conflict as a result of economic exploitation. Would belief in relativism-the idea that everyone creates his or her own truth-increase or decrease someone’s motivation to analyze these three viewpoints and pursue the question of society’s evolution? Explain your response.

3. Read each of the following passages, decide how reasonable it is, and explain your thinking. 
a. People who believe that "everyone creates his or her own truth" should never argue with anyone about anything. If they do, they are being inconsistent. 
b. Motivation to do anything depends on the belief that it has not yet been done. Everyone who loses something precious, say a diamond ring, will search diligently and even desperately until it is found. But only a fool would continue searching for it after it was found. It is no different with other kinds of searches, such as the search for truth. Once we think we have it, we stop looking.

4. For years grade school students faced this question on their science tests: "True or False-The famous rings of the planet Saturn are composed of solid material." If the students marked "true," they lost credit, because the "truth" was that Saturn’s rings were composed of gas or dust. Then, in 1973, radar probes revealed that all those wrong answers had been right. Saturn’s rings are, in fact, composed of solid matter.12 This confusing case seems to suggest that the truth changed. Did it really? Explain.

5. The scene is a campus security office, where two students are being questioned. A few minutes earlier, they were engaged in a fistfight in the cafeteria. The campus police ask them again and again how the fight started. The stories conflict. Because each student seems genuinely convinced that the other one was the aggressor and there were no witnesses, the campus police have no hope of discovering the truth. But is there a truth to discover? Or are there two truths, one for each student’s story? What light does the chapter shed on these questions?

6. A strange phenomenon that affects a tiny number of the world’s inhabitants has interested psychologists for some time. It occurs during what Norwegians call the "murky time," the two months each year during which areas above the Arctic Circle experience almost unrelieved darkness. The effects on people have been discovered to be unfortunate, even dangerous. At worst, people experience severe tenseness, restlessness, fear, a preoccupation with thoughts of death and even suicide. At best, they experience an inability to concentrate, fatigue, a lack of enthusiasm for anything, suspicion, and jealousy. Part of the cause is seen as lack of sleep. Accustomed to day and night, people become confused by constant darkness. This phenomenon poses an interesting test of truth. Would it be proper to say the phenomenon was true before it was recognized and acknowledged by psychologists? Or did it become true only when they became aware of it? And what of your relationship to the phenomenon? Before you became aware of it for the first time, whether reading it here or elsewhere, it was not "true to you." But did that make it any less true? Explain in light of this chapter.

7. Evaluate the following dialogues in light of what you learned in this chapter. If you lack sufficient knowledge to judge the issue, do some research. 
a. 
    Martha: I don’t care what the courts say about abortion-I’m convinced it’s murder because the fetus is a human being. 
    Marian: If you want to believe that, fine. Just don’t impose your beliefs on others and prevent them from exercising their rights.
    Martha: You don’t seem to understand. It’s not just a fetus in my uterus that’s human but the fetus in the uterus of every pregnant woman.
    Marian: Nonsense. You have no right to classify what exists in someone else’s uterus. That’s her business. You should mind your own business.
b. 
    Barbi: Television shows about suicide should not be aired.
    Ken: Why?
    Barbi: Because they cause people to commit suicide.
    Ken: That’s ridiculous. How can a drama or documentary that shows the tragedy of suicide cause people to commit suicide?
    Barbi: I don’t know how it happens. Maybe some people have thoughts of suicide already and the show reinforces them. Or maybe they focus on the act of suicide and lose sight of the tragedy. All I know is that attempted suicides increase after the airing of such shows.
c.
    Mabel: I notice that when you get a newspaper you immediately turn to the astrology column. Do you really believe that nonsense?
    Alphonse: It’s not nonsense. The planets exercise a powerful influence on our lives; their positions in the heavens at the time of our birth can shape our destiny.
    Mabel: I can’t believe I’m hearing such slop from a science major.
    Alphonse: What you fail to understand is that astrology is science, one of the most ancient sciences at that.
d.
    Jake: What did you think of the chapter "What Is Truth?"
    Rocky: It’s stupid.
    Jake: What do you mean?
    Rocky: It contradicts Chapter 1.
    Jake: I didn’t get that impression. Where’s the contradiction?
    Rocky: In Chapter 1 the author says that we should strive to be individuals and think for ourselves. Now he says that his idea about truth is OK and ours isn’t and that we should follow his. That’s a contradiction.

8.Group discussion exercise: How many times have you been certain something was true, only to find out later that it was not? Discuss those experiences with two or three classmates. Be prepared to share the most dramatic and interesting experiences with the rest of the class.

\section{A Difference of Opinion}

The following passage summarizes an important difference of opinion. After reading the statement, use the library and/or the Internet and find what knowledgeable people have said about the issue. Be sure to cover the entire range of views. Then assess the strengths and weaknesses of each. If you conclude that one view is entirely correct and the others are mistaken, explain how you reached that conclusion. If, as is more likely, you find that one view is more insightful than the others but that they all make some valid points, construct a view of your own that combines the insights from all views and explain why that view is the most reasonable of all. Present your response in a composition or an oral report, as your instructor specifies.

> Who is responsible for the fiscal crisis of 2008? This issue continues to be central to overcoming the consequences of the crisis and to ensuring that it does not recur. Commentators are divided on the cause. Some claim it is was the policies of George W. Bush’s administration; others, the policies of the Clinton administration; others, the greed of Wall Street executives. Many point, instead, to congressional pressure on banks, during the 1990s, to give loans to people who could not afford to repay them. Still others say the crisis originated during the Carter administration, specifically in the Community Reinvestment Act of 1977.

Begin your analysis by conducting a Google search using the terms "Community Reinvestment Act," "causes financial crisis," and "subprime mortgage crisis."

\chapter{What Does It Mean to Know?}

Sally looks up from her composition and asks her roommates, "How do you spell embarrass?"

Nancy says, "I’m not sure. I think it has a double r and a double s. Oh, I really don’t know."

Marie smiles her smug smile. "I guess spelling isn’t your cup of tea, Nancy. The correct spelling is e-m-b-a-r-a-s-s. Only one r."

By this time Sally has already opened her dictionary. "Might as well check to be sure," she says. "Let’s see, embargo, embark . . . here it is, embarrass. Double r and double s. You were right, Nancy."

Let’s consider what happened more closely. Marie knew the answer, but she was wrong. Nancy didn’t know, but she was right. Confusing. What kind of thing can this knowing be? When you’re doing it, you’re not doing it. And when you aren’t, you are.

Fortunately, it only appears to be that way. The confusion arises because the feelings that accompany knowing can be present when we don’t know. Marie had those feelings. She no longer wondered or experienced any confusion; she was sure of the answer. Yet she was mistaken.

\section{Requirements of Knowing}

Nancy was in a better position than Marie because she answered correctly. Yet she didn’t know, for knowing involves more than having the right answer. It also involves the realization that you have it.

The issue, of course, may not always be as simple as the spelling of a word. It may require understanding numerous details or complex principles or steps in a process. (It may also involve a skill-knowing how to do something. But that is a slightly different use of the word than concerns us here.)

Knowing usually implies something else, too-the ability to express what is known and how we came to know it. This, however, is not always so. We may not be able to express our knowledge in words. The best we may be able to say is "I just know, that’s all" or "I know because I know." Yet these replies are feeble and hardly satisfy those who wish to verify our knowledge or acquire it.

\section{Testing Your Own Knowledge}

Following are some items of "common knowledge." Determine how many you already know, and then decide, if possible, how you came to know each. Complete this informal inventory before continuing with the chapter.

1. Women are nurturing but men are not.
2. African Americans had little or no part in settling the American West.
3. Expressing anger has the effect of reducing it and making us feel better.
4. The Puritans were "prim, proper, and prudish prigs."
5. Before Columbus arrived in the New World, Native Americans lived in peace with one another and in respectful harmony with the environment.
6. Alfred Kinsey’s research on human sexuality is scrupulously scholarly and objective.
7. Employers import unskilled labor from other countries to save money.
8. The practice of slavery originated in colonial America.

It would be surprising if you did not think you knew most of these items. After all, many writers have written about them, and they are widely accepted as conventional wisdom. But let’s look a little more closely at each of them.

1. Barbara Risman became curious about this idea and decided to study it further. Her findings challenged the conventional wisdom. Apparently, men who are responsible for caring for children or elderly parents display the same nurturing traits usually associated with women. She concluded that these traits are as dependent on one’s role in life as on one’s sex.
2. The facts contradict what is known. For example, 25 percent of the cowboys in Texas cattle drives were African American, as were 60 percent of original settlers of Los Angeles. The reason these facts are not more widely known is probably because of scholarly omission of information about African Americans from the history books.
3. Conventional wisdom again is wrong. After reviewing the evidence about anger, Carol Tavris concludes, "The psychological rationales for ventilating anger do not stand up under experimental scrutiny. The weight of the evidence indicates precisely the opposite: expressing anger makes you angrier, solidifies an angry attitude, and establishes a hostile habit. If you keep quiet about momentary irritations and distract yourself with pleasant activity until your fury simmers down, chances are you will feel better, and feel better faster, than if you let yourself go in a shouting match."
4. Although the Puritans did hold that sex is rightly reserved for marriage, they did not hesitate to talk openly about the subject and were not prudish within marriage. The problem seems to be that people confuse the Puritans with the Victorians.
5. This is pure myth. Few tribes were completely peaceful, and many not only were warlike but slaughtered women and children and tortured their captives. Some tribes also offered human sacrifices, murdered the elderly, and practiced cannibalism. As to their alleged harmonious respect for nature, many tribes deforested the land and wantonly killed whole herds of animals.
\end{document}
