# Beyond Feelings A Guide to Critical Thinking

NINTH EDITION

Vincent Ryan Ruggiero

Professor Emeritus of Humanities State University of New York, Delhi

To the memory of Howard Trumble, whose quiet practice of the skills detailed in this book was an inspiration to me, to his family, and to all who knew him.

## Contents

Preface ix
Introduction
PART ONE The Context
Chapter 1 Who Are You?
    The Influence of Time and Place
    The Influence of Ideas
    The Influence of Mass Culture
    The “Science” of Manipulation
    The Influence of Psychology
    Becoming an Individual
Chapter 2 What Is Critical Thinking?
    Mind, Brain, or Both?
    Critical Thinking Defined
    Characteristics of Critical Thinkers
    The Role of Intuition
    Basic Activities in Critical Thinking
    Critical Thinking and Writing
    Critical Thinking and Discussion
    Avoiding Plagiarism
Chapter 3 What Is Truth?
    Where Does It All Begin?
    Imperfect Perception
    Imperfect Memory
    Deficient Information
    Even the Wisest Can Err
    Truth Is Discovered, Not Created
    Understanding Cause and Effect
Chapter 4 What Does It Mean to Know?
    Requirements of Knowing
    Testing Your Own Knowledge
    How We Come to Know
    Why Knowing Is Difficult
    A Cautionary Tale
    Is Faith a Form of Knowledge?
    Obstacles to Knowledge
Chapter 5 How Good Are Your Opinions?
    Opinions Can Be Mistaken
    Opinions on Moral Issues
    Even Experts Can Be Wrong
    Kinds of Errors
    Informed Versus Uninformed Opinion
    Forming Opinions Responsibly
Chapter 6 What Is Evidence?
    Kinds of Evidence
    Evaluating Evidence
    What Constitutes Sufficient Evidence?
Chapter 7 What Is Argument?
    The Parts of an Argument
    Evaluating Arguments
    More Difficult Arguments
PART TWO The Pitfalls
Chapter 8 The Basic Problem: “Mine Is Better”
    Egocentric People
    Ethnocentric People
    Controlling “Mine-Is-Better” Thinking
Chapter 9 Errors of Perspective
    Poverty of Aspect
    Unwarranted Assumptions
    The Either/Or Outlook
    Mindless Conformity
    Absolutism
    Relativism
    Bias for or Against Change
Chapter 10 Errors of Procedure
    Biased Consideration of Evidence
    Double Standard
    Hasty Conclusion
    Overgeneralization and Stereotyping
    Oversimplification
    The Post Hoc Fallacy
Chapter 11 Errors of Expression
    Contradiction
    Arguing in a Circle
    Meaningless Statement
    Mistaken Authority
    False Analogy
    Irrational Appeal
Chapter 12 Errors of Reaction
    Automatic Rejection
    Changing the Subject
    Shifting the Burden of Proof
    Straw Man
    Attacking the Critic
Chapter 13 The Errors in Combination
    Errors of Perspective
    Errors of Procedure
    Errors of Expression
    Errors of Reaction
    Sample Combinations of Errors
    A Sensible View of Terminology
PART THREE A Strategy
Chapter 14 Knowing Yourself
    Critical Thinking Inventory
    Using Your Inventory
    Challenge and Reward
Chapter 15 Being Observant
    Observing People
    Observation in Science and Medicine
    The Range of Application
    Becoming More Observant
    Reflecting on Your Observations
Chapter 16 Selecting an Issue
    The Basic Rule: Less Is More
    How to Limit an Issue
    Sample Issue: Pornography
    Sample Issue: Boxing
    Sample Issue: Juvenile Crime
    Narrowing the Issue Further
Chapter 17 Conducting Inquiry
    Working with Inconclusive Results
    Where to Look for Information
    Keeping Focused
    How Much Inquiry Is Enough?
    Managing Lengthy Material
Chapter 18 Forming a Judgment
    Evaluating Evidence
    Evaluating Your Sources’ Arguments
    Making Important Distinctions
    Expressing Judgments
Chapter 19 Persuading Others
    Guidelines for Persuasion
    An Unpersuasive Presentation
    A Persuasive Presentation
Notes
Index

## Preface

When the first edition of this book appeared in 1975, the dominant intellectual focus was still subjectivity, feelings. That focus, the legacy of the 1960s, was originally a necessary reaction to the rationalism and behaviorism that preceded it. It declared, in effect: “People are not robots. They are more than the sum total of their physiology. They have hopes, dreams, emotions. No two humans are alike—each has a special perspective, a unique way of perceiving the world. And any view of humanity that ignores this subjective side is a distortion.”

Yet, despite its value, the focus on feelings went too far. Like many other movements, what began as a reaction against an extreme view became an extreme view itself. The result of that extremism was the neglect of thinking. This book was designed to answer that neglect. The introduction to the first edition explained its rationale as follows:

>The emphasis on subjectivity served to correct a dangerous oversimplification. But it is the kind of reaction that cannot be sustained for long without causing an even worse situation—the neglect of thinking. Worse for two reasons. First, because we live in an age of manipulation. Armies of hucksters and demagogues stand ready with the rich resources of psychology to play upon our emotions and subconscious needs to persuade us that superficial is profound, harmful is beneficial, evil is virtuous. And feelings are especially vulnerable to such manipulation.

>Secondly, because in virtually every important area of modern life—law, medicine, government, education, science, business, and community affairs—we are beset with serious problems and complex issues that demand careful gathering and weighing of facts and informed opinions, thoughtful consideration of various conclusions or actions, and judicious selection of the best conclusion or most appropriate action. . . .

>[Today’s college student] has been conditioned not to undervalue subjectivity, but to overvalue it. And so he does not need to have his feelings indulged. Rather, he needs to be taught how to sort out his feelings, decide to what extent they have been shaped by external influences, and evaluate them carefully when they conflict among themselves or with the feelings of others. In short, he needs to be taught to think critically.

>There is an unfortunate tendency among many to view feeling and thought as mutually exclusive, to force a choice between them. If we focus on one, then in their view we must reject the other. But this is mistaken. Feeling and thought are perfectly complementary. Feeling, being more spontaneous, is an excellent beginning to the development of conclusions. And thought, being more deliberate, provides a way to identify the best and most appropriate feeling. Both are natural.

>Thinking, however, is less automatic than feeling. To do it well demands a systematic approach and guided practice.

The general attitude toward thinking has changed considerably since the mid-1970s. The view that critical thinking is an important skill to which education should give prominence is no longer a minority view. Hundreds of voices have joined the chorus calling for the addition of critical thinking objectives to existing courses and even the creation of special courses in thinking. There is little disagreement that the challenges of the new millennium demand minds that can move beyond feelings to clear, impartial, critical problem solving and decision making.

### Features of This Edition

This edition of Beyond Feelings retains the basic organization of previous editions. The first section explains the psychological, philosophical, and social context in which critical thinking takes place and describes the habits and attitudes that enhance such thinking. The second section helps students recognize and overcome common errors in thinking. The third section provides a step-by-step strategy for dealing with issues.

Within the overall design, however, I have made a number of changes, most in response to the helpful suggestions of reviewers.
In Chapter 1, a new section—“The Influence of Ideas”—has been added.
In Chapter 3, a new section—“Understanding Cause and Effect”— has been added.
In Chapter 15, new examples of the value of observation have been added.
In Chapter 17, the subsection “Evaluate your information sources” has been expanded.
A number of new “Difference of Opinion” exercises have been added.

As in the past, I have attempted to follow George Orwell’s sage advice: “Never use a foreign phrase, a scientific word, or a jargon word if you can think of an everyday English equivalent.” This is not always easy. When logicians are taught terms such as argumentum ad hominem, non sequitur, and “affirming the consequent,” they naturally want to use them. Arguments for doing so urge themselves upon us: for example, “These are the most precise terms. Don’t join the ranks of the coddlers and deprive students of them.” In weak moments I succumb to this appeal. (Until the previous edition, for example, I included the term enthymeme. Mea culpa . . . there I go again.) But is the precision of such terms the real reason for my wanting to use them? Is it not possible that we professors enjoy parading our knowledge or that we are reluctant to spare our students the struggle we were forced to undergo (“We suffered, so they should too”)? It seems to me that modern culture already provides too many impediments to critical thinking for us to add more.

Is it possible to carry this plain language commitment too far? Yes, and some will think I have done so in avoiding the term inferences and speaking instead of conclusions. But I respectfully disagree. Lexicographers point out that the distinction between these terms is extremely subtle, so it seems more reasonable not to devote time to it. Also, I avoid using the term values whenever possible for a somewhat different reason. The word value is so associated with relativism that its use in this context can undermine the crucial idea that arguments differ in quality. For many students, the word value triggers the thought, “Everyone has a right to his or her values; mine are right for me, and though they may need ‘clarification’ from time to time, they are never to be questioned.” This thought impedes critical thinking.

## Introduction

Beyond Feelings is designed to introduce you to the subject of critical thinking. The subject may be new to you because it has not been emphasized in most elementary and secondary schools. In fact, until fairly recently, most colleges gave it little attention. For the past four decades, the dominant emphasis has been on subjectivity rather than objectivity, on feeling rather than on thought.

Over the past several decades, however, a number of studies of America’s schools have criticized the neglect of critical thinking, and a growing number of educators and leaders in business, industry, and the professions have urged the development of new courses and teaching materials to overcome that neglect.

It is no exaggeration to say that critical thinking is one of the most important subjects you will study in college regardless of your academic major. The quality of your schoolwork, your efforts in your career, your contributions to community life, your conduct of personal affairs—all will depend on your ability to solve problems and make decisions.

The book has three main sections. The first, “The Context,” will help you understand such important concepts as individuality, critical thinking, truth, knowledge, opinion, evidence, and argument and overcome attitudes and ideas that obstruct critical thinking. The second section, “The Pitfalls,” will teach you to recognize and avoid the most common errors in thinking. The third section, “A Strategy,” will help you acquire the various skills used in addressing problems and issues. This section includes tips on identifying and overcoming your personal intellectual weaknesses as well as techniques for becoming more observant, clarifying issues, conducting inquiries, evaluating evidence, analyzing other people’s views, and making sound judgments.

At the end of each chapter, you will find a number of applications to challenge your critical thinking and help you exercise your skills. These applications cover problems and issues both timely and timeless. The final application in each of the first thirteen chapters invites you to examine an especially important issue about which informed opinion is divided.

Students sometimes get the idea that a textbook must be read page by page and that reading ahead violates some unwritten rule. This notion is mistaken. Students’ background knowledge varies widely; what one student knows very well, another knows only vaguely and a third is totally unfamiliar with. Any time you need or want to look ahead to an explanation in a later chapter, by all means do so. Let’s say you make a statement and a friend says, “That’s relativism, pure and simple.” If you aren’t sure exactly what she means, go to the index, look up “relativism,” proceed to the appropriate page, and find out.

Looking ahead is especially prudent in the case of concepts and procedures relevant to the end-of-chapter applications. One such concept is plagiarism. If you are not completely clear on what constitutes plagiarism, why it is unacceptable, and how to avoid it, take a few minutes right now to learn. Look for the section “Avoiding Plagiarism” toward the end of the Chapter 2. Similarly, if you are not as skilled as you would like to be doing library or Internet research, it would be a good idea to read Chapter 17 now. Doing so could save you a great deal of time and effort completing homework assignments.

# PART ONE The Context

Anyone who wishes to master an activity must first understand its tools and rules. This is as true of critical thinking as it is of golf, carpentry, flying a plane, or brain surgery. In critical thinking, however, the tools are not material objects but concepts, and the rules govern mental rather than physical performance.

This first section explores seven important concepts—individuality, critical thinking, truth, knowledge, opinion, evidence, and argument— with a chapter devoted to each. Most of these concepts are so familiar that you may be inclined to wonder whether there is any point to examining them. The answer is yes, for three reasons. First, much of what is commonly believed about these concepts is mistaken. Second, who ever examines them carefully is always rewarded with fresh insights. Third, the more thorough your knowledge of these concepts, the more proficient you will be in your thinking.

## CHAPTER 1 Who Are You?

Suppose someone asked, “Who are you?” It would be simple enough to respond with your name. But if the person wanted to know the entire story about who you are, the question would be more difficult to answer. You’d obviously have to give the details of your height, age, and weight. You’d also have to include all your sentiments and preferences, even the secret ones you’ve never shared with anyone—your affection for your loved ones; your desire to please the people you associate with; your dislike of your older sister’s husband; your allegiance to your favorite beverage, brand of clothing, and music.

Your attitudes couldn’t be overlooked either—your impatience when an issue gets complex, your aversion to certain courses, your fear of high places and dogs and speaking in public. The list would go on. To be complete, it would have to include all your characteristics—not only the physical but also the emotional and intellectual.

To provide all that information would be quite a chore. But suppose the questioner was still curious and asked, “How did you get the way you are?” If your patience were not yet exhausted, chances are you’d answer something like this: “I’m this way because I choose to be, because I’ve considered other sentiments and preferences and attitudes and have made my selections. The ones I have chosen fit my style and personality best.” That answer is natural enough, and in part it’s true. But in a larger sense, it’s not true. The impact of the world on all of us is much greater than most of us realize.

### The Influence of Time and Place

Not only are you a member of a particular species, Homo sapiens, but you also exist at a particular time in the history of that species and in a particular place on the planet. That time and place are defined by specific circumstances, understandings, beliefs, and customs, all of which limit your experience and influence your thought patterns. If you had lived in America in colonial times, you likely would have had no objection to the practice of barring women from serving on a jury, entering into a legal contract, owning property, or voting. If you had lived in the nineteenth century, you would have had no objection to young children being denied an education and being hired out by their parents to work sixteen hours a day, nor would you have given any thought to the special needs of adolescence. (The concept of adolescence was not invented until 1904.)<sup>1</sup>

If you had been raised in the Middle East, you would stand much closer to people you converse with than you do in America. If you had been raised in India, you might be perfectly comfortable having your parents choose your spouse for you. If your native language were Spanish and your knowledge of English modest, you probably would be confused by some English colloquialisms. James Henslin offers two amusing examples of such confusion: Chevrolet Novas initially sold very poorly in Mexico because no va in Spanish means “it doesn’t work”; and Perdue chickens were regarded with a certain suspicion (or worse) because the company’s slogan—”It takes a tough man to make a tender chicken”— became in Spanish “It takes an aroused man to make a chicken affectionate.”<sup>2</sup>

People who grow up in Europe, Asia, or South America have very different ideas of punctuality. As Daniel Goleman explains, “Five minutes is late but permissible for a business appointment in the U.S., but thirty minutes is normal in Arab countries. In England five to fifteen minutes is the ‘correct’ lateness for one invited to dinner; an Italian might come two hours late, an Ethiopian still later, a Javanese not at all, having accepted only to prevent his host’s losing face.”<sup>3</sup> A different ethnic origin would also mean different tastes in food. Instead of craving a New York Strip steak and french fries, you might crave “raw monkey brains” or “camel’s milk cheese patties cured in dry camel’s dung” and washed down with “warm camel’s blood.”<sup>4</sup> Sociologist Ian Robertson summed up the range of global dietary differences succinctly: “Americans eat oysters but not snails. The French eat snails but not locusts. The Zulus eat locusts but not fish. The Jews eat fish but not pork. The Hindus eat pork but not beef. The Russians eat beef but not snakes. The Chinese eat snakes but not people. The Jalé of New Guinea find people delicious.”<sup>5</sup> [Note: The reference to Hindus is mistaken.]

To sum up, living in a different age or culture would make you a different person. Even if you rebelled against the values of your time and place, they still would represent the context of your life—in other words, they still would influence your responses.

### The Influence of Ideas<sup>6</sup>

When one idea is expressed, closely related ideas are simultaneously conveyed, logically and inescapably.<sup>7</sup> In logic, this kinship is expressed by the term sequitur, Latin for “it follows.” (The converse is non sequitur, “it does not follow.”)<sup>8</sup>

Consider, for example, the idea that many teachers and parents express to young children as a way of encouraging them: “If you believe in yourself, you can succeed at anything.” From this it follows that nothing else but belief—neither talent nor hard work—is necessary for success. The reason the two ideas are equivalent is that their meanings are inseparably linked.

In addition to conveying ideas closely linked to it in meaning, an idea can imply other ideas. For example, the idea that there is no real difference between virtue and vice implies that people should not feel bound by common moral standards. Samuel Johnson had this implication in mind when he said: “But if he does really think that there is no distinction between virtue and vice, why, Sir, when he leaves our houses let us count our spoons.”

If we were fully aware of the closely linked meanings and implications of the ideas we encounter, we could easily sort out the sound ones from the unsound, the wise from the foolish, and the helpful from the harmful. But we are seldom fully aware. In many cases, we take ideas at face value and embrace them with little or no thought of their associated meanings and implications. In the course of time, our actions are shaped by those meanings and implications, whether we are aware of them or not.

To appreciate the influence of ideas in people’s lives, consider the series of events set in motion by an idea that was popular in psychology more than a century ago and whose influence continues to this day—the idea that “intelligenceisgeneticallydeterminedandcannotbeincreased.”

> That idea led researchers to devise tests that measure intelligence. The most famous (badly flawed) test determined that the average mental age of white American adults was 13 and that, among immigrants, the average Russian’s mental age was 11.34; the average Italian’s, 11.01; the average Pole’s, 10.74; and the average mental age of “Negroes,” 10.41.

> Educators read the text results and thought, “Attempts to raise students’ intelligence are pointless,” so they replaced academic curricula with vocational curricula and embraced a methodology that taught students facts but not the process of judgment.

> Legislators read the test results and decided “We’ve got to do something to keep intellectually inferior people from entering the country,” so they revised immigration laws to discriminate against southern and central Europeans.

> Eugenicists, who had long been concerned about the welfare of the human species,saw the tests as a grave warning.They thought, “If intelligence can not be increased, we must find ways of encouraging reproduction among people of higher intelligence and discouraging it among those of lower intelligence.”

> The eugenicists’ concern inspired a variety of actions. Margaret Sanger’s Planned Parenthood urged the lower classes to practice contraception. Others succeeded in legalizing promoted forced sterilization, notably in Virginia. The U.S. Supreme Court upheld the Virginia law with Justice Oliver Wendell Holmes, Jr. declaring, “Three generations of imbeciles are enough.”<sup>9</sup> Over the next five decades 7,500 women, including “unwed mothers, prostitutes, petty criminals and children with disciplinary problems” were sterilized.<sup>10</sup> In addition, by 1950 over 150,000 supposedly “defective” children, many relatively normal, were held against their will in institutions. They “endured isolation, overcrowding, forced labor, and physical abuse including lobotomy, electroshock, and surgical sterilization.”<sup>11</sup>

> Meanwhile, business leaders read the test results and decided, “We need policies to ensure that workers leave their minds at the factory gate and perform their assigned tasks mindlessly.” So they enacted those policies. Decades later,when Edwards Deming proposed his “quality control” ideas for involving workers in decision making, business leaders remembered those test results and ignored Deming’s advice. (In contrast, the Japanese welcomed Deming’s ideas; as a result, several of their industries surged ahead of their American competition.)

These are the most obvious effects of hereditarianism but they are certainly not the only ones. Others include discrimination against racial and ethnic minorities and the often-paternalistic policies of government offered in response. (Some historians also link hereditarianism to the genocide that occurred in Nazi Germany.)

The innumerable ideas you have encountered will affect your beliefs and behavior in similar ways––sometimes slightly, at other times profoundly. And this can happen even if you have not consciously embraced the ideas.

### The Influence of Mass Culture

In centuries past, family and teachers were the dominant, and sometimes the only, influence on children. Today, however, the influence exerted by mass culture (the broadcast media, newspapers, magazines, Internet and popular music) often is greater.

By age 18 the average teenager has spent 11,000 hours in the classroom and 22,000 hours in front of the television set. He or she has had perhaps 13,000 school lessons yet has watched more than 750,000 commercials. By age thirty-five the same person has had fewer than 20,000 school lessons yet has watched approximately 45,000 hours of television and close to 2 million commercials.

What effects does mass culture have on us? To answer, we need only consider the formats and devices commonly used in the media. Modern advertising typically bombards the public with slogans and testimonials by celebrities. This approach is designed to appeal to emotions and create artificial needs for products and services. As a result, many people develop the habit of responding emotionally, impulsively, and gullibly to such appeals. They also tend to acquire values very different from those taught in the home and the school. Ads often portray play as more fulfilling than work, self-gratification as more desirable than self-control, and materialism as more meaningful than idealism.

Television programmers use frequent scene shifts and sensory appeals such as car crashes, violence, and sexual encounters to keep audience interest from diminishing. Then they add frequent commercial interruptions. This author has analyzed the attention shifts that television viewers are subjected to. In a dramatic program, for example, attention shifts might include camera angle changes; shifts in story line from one set of characters (or subplot) to another, or from a present scene to a past scene (flashback), or to fantasy; and shifts to “newsbreaks,” to commercial breaks, from one commercial to another, and back to the program. Also included might be shifts of attention that occur within commercials. I found as many as 78 shifts per hour, excluding the shifts within commercials. The number of shifts within commercials ranged from 6 to 54 and averaged approximately 17 per fifteen-second commercial. The total number of attention shifts came out to over 800 per hour, or over 14 per minute.

This manipulation has prevented many people from developing a mature attention span. They expect the classroom and the workplace to provide the same constant excitement they get from television. That, of course, is an impossible demand, and when it isn’t met they call their teachers boring and their work unfulfilling. Because such people seldom have the patience to read books that require them to think, many publishers have replaced serious books with light fare written by celebrities.

Even when writers of serious books do manage to become published authors, they are often directed to give short, dramatic answers during promotional interviews, sometimes at the expense of accuracy. A man who coaches writers for talk shows offered one client this advice: “If I ask you whether the budget deficit is a good thing or a bad thing, you should not say, ‘Well, it stimulates the economy but it passes on a burden.’ You have to say ‘It’s a great idea!’ or ‘It’s a terrible idea!’ It doesn’t matter which.”<sup>12</sup> (Translation: ”Don’t give a balanced answer. Give an oversimplified one because it will get you noticed.”)

Print journalism is also in the grip of sensationalism. As a newspaper editor observed, “Journalists keep trying to find people who are at 1 or at 9 on a scale of 1 to 10 rather than people at 3 to 7 [the more moderate positions] where most people actually are.”<sup>13</sup> Another journalist claims, “News is now becoming more opinion than verified fact. Journalists are slipping into entertainment rather than telling us the verified facts we need to know.”<sup>14</sup>

Today’s politicians often manipulate people more offensively than do journalists. Instead of expressing their thoughts, some politicians find out what people think and pretend to share their ideas. Many politicians hire people to conduct polls and focus groups to learn what messages will “sell.” They even go so far as to test the impact of certain words—that is why we hear so much about “trust,” “family,” “character,” and “values” these days. Political science professor Larry Sabato says that during the Clinton impeachment trial, the president’s advisors used the term private lives over and over—James Carville used it six times in one four-minute speech—because they knew it could persuade people into believing the president’s lying under oath was of no great consequence.<sup>15</sup>

### The “Science” of Manipulation

Attempts to influence the thoughts and actions of others are no doubt as old as time, but manipulation did not become a science until the early twentieth century, when Ivan Pavlov, a Russian professor of psychology, published his research on conditioned (learned) reflexes. Pavlov found that by ringing a bell when he fed a dog, he could condition the dog to drool at the sound of the bell even when no food was presented. An American psychologist, John Watson, was impressed with Pavlov’s findings and applied them to human behavior. In Watson’s most famous experiment, he let a baby touch a laboratory rat. At first, the baby was unafraid. But then Watson hit a hammer against metal whenever the baby reached out to touch the rat, and the baby became frightened and cried. In time, the baby cried not only at the sight of the rat but also at the sight of anything furry, such as a stuffed animal. Watson’s work earned him the title “father of behaviorism.”

Less well known is Watson’s application of behaviorist principles to advertising. He spent the latter part of his career working for advertising agencies and soon recognized that the most effective appeal to consumers was not to the mind but to the emotions. He advised advertisers to “tell [the consumer] something that will tie him up with fear, something that will stir up a mild rage, that will call out an affectionate or love response, or strike at a deep psychological or habit need.” His attitude toward the consumer is perhaps best indicated by a statement he made in a presentation to department store executives: “The consumer is to the manufacturer, the department stores and the advertising agencies, what the green frog is to the physiologist.”<sup>16</sup>

Watson introduced these strategies in the 1920s and 1930s, the age of newspapers and radio. Since the advent of television, these advertising strategies have grown more sophisticated and effective, so much so that many individuals and groups with political and social agendas have adopted them. The strategies work for a number of reasons, the chief one being people’s conviction that they are impervious to manipulation. This belief is mistaken, as many researchers have demonstrated. For example, Solomon Asch showed that people’s reactions can be altered simply by changing the order of words in a series. He asked study participants to evaluate a person by a series of adjectives. When he put positive adjectives first—”intelligent, industrious, impulsive, critical, stubborn, envious”— the participants gave a positive evaluation. When he reversed the order, with “envious” coming first and “intelligent” last, they gave a negative evaluation.<sup>17</sup>

Similarly, research has shown that human memory can be manipulated. The way a question is asked can change the details in a person’s memory and even make a person remember something that never happened!<sup>18</sup>

Of course, advertisers and people with political or social agendas are not content to stimulate emotions and/or plant ideas in our minds. They also seek to reinforce those impressions by repeating them again and again. The more people hear a slogan or talking point, the more familiar it becomes. Before long, it becomes indistinguishable from ideas developed through careful thought. Sadly, “the packaging is often done so effectively that the viewer, listener, or reader does not make up his own mind at all. Instead, he inserts a packaged opinion into his mind, somewhat like inserting a DVD into a DVD player. He then pushes a button and ‘plays back’ the opinion whenever it seems appropriate to do so. He has performed acceptably without having had to think.”<sup>19</sup> Many of the beliefs we hold dearest and defend most vigorously may have been planted in our minds in just this way.

Many years ago, Harry A. Overstreet noted that “a climate of opinion, like a physical climate, is so pervasive a thing that those who live within it and know no other take it for granted.”<sup>20</sup> The rise of mass culture and the sophisticated use of manipulation have made this insight more relevant today than ever.

### The Influence of Psychology

The social and psychological theories of our time also have an impact on our beliefs. Before the past few decades, people were urged to be self-disciplined, self-critical, and self-effacing. They were urged to practice self-denial, to aspire to self-knowledge, to behave in a manner that ensured they maintained self-respect. Self-centeredness was considered a vice. “Hard work,” they were told, “leads to achievement, and that in turn produces satisfaction and self-confidence.” By and large, our grandparents internalized those teachings. When they honored them in their behavior, they felt proud; when they dishonored them, they felt ashamed.

Today the theories have been changed—indeed, almost exactly reversed. Self-esteem, which nineteenth-century satirist Ambrose Bierce defined as “an erroneous appraisement,” is now considered an imperative. Self-centeredness has been transformed from vice into virtue, and people who devote their lives to helping others, people once considered heroic and saintlike, are now said to be afflicted with “a disease to please.” The formula for success and happiness begins with feeling good about ourselves. Students who do poorly in school, workers who don’t measure up to the challenges of their jobs, substance abusers, lawbreakers—all are typically diagnosed as deficient in self-esteem.

In addition, just as our grandparents internalized the social and psychological theories of their time, so most contemporary Americans have internalized the message of self-esteem. We hear people speak of it over coffee; we hear it endlessly invoked on talk shows. Challenges to its precepts are usually met with disapproval.

But isn’t the theory of self-esteem self-evident? No. A negative perception of our abilities will, of course, handicap our performance. Dr. Maxwell Maltz explains the amazing results one educator had in improving the grades of schoolchildren by changing their self-images. The educator had observed that when the children saw themselves as stupid in a particular subject (or stupid in general), they unconsciously acted to confirm their self-images. They believed they were stupid, so they acted that way. Reasoning that it was their defeatist attitude rather than any lack of ability that was undermining their efforts, the educator set out to change their self-images. He found that when he accomplished that, they no longer behaved stupidly! Maltz concludes from this and other examples that our experiences can work a kind of self-hypnotism on us, suggesting a conclusion about ourselves and then urging us to make it come true.<sup>21</sup>

Many proponents of self-esteem went far beyond Maltz’s demonstration that self-confidence is an important ingredient in success. They claimed that there is no such thing as too much self-esteem. Research does not support that claim. For example, Martin Seligman, an eminent research psychologist and founder of the movement known as positive psychology, cites significant evidence that, rather than solving personal and social problems, including depression, the modern emphasis on self-esteem causes them.<sup>22</sup>

Maltz’s research documents that lack of confidence impedes performance, a valuable insight. But such research doesn’t explain why the more global concept of self-esteem has become so dominant. The answer to that question lies in the popularization of the work of humanistic psychologists such as Abraham Maslow. Maslow described what he called the hierarchy of human needs in the form of a pyramid, with physiological needs (food and drink) at the foundation. Above them, in ascending order, are safety needs, the need for belongingness and love, the need for esteem and approval, and aesthetic and cognitive needs (knowledge, understanding, etc.). At the pinnacle is the need for self-actualization, or fulfillment of our potential. In Maslow’s view, the lower needs must be fulfilled before the higher ones. It’s easy to see how the idea that self-esteem must precede achievement was derived from Maslow’s theory.

Other theories might have been adopted, however. A notable one is Austrian psychiatrist Viktor Frankl’s, which was advanced at roughly the same time as Maslow’s and was based on both Frankl’s professional practice and his experiences in Hitler’s concentration camps. Frankl argues that one human need is higher than self-actualization: self-transcendence, the need to rise above narrow absorption with self. According to Frankl, “the primordial anthropological fact [is] that being human is being always directed, and pointing to something or someone other than oneself: to a meaning to fulfill or another human being to encounter, a cause to serve or a person to love.” A person becomes fully human “by forgetting himself and giving himself, overlooking himself and focusing outward.”

Making self-actualization (or happiness) the direct object of our pursuit, in Frankl’s view, is ultimately self-defeating; such fulfillment can occur only as “the unintended effect of self-transcendence.”<sup>23</sup> The proper perspective on life, Frankl believes, is not what it can give to us, but what it expects from us; life is daily—even hourly—questioning us, challenging us to accept “the responsibility to find the right answer to its problems and to fulfill the tasks which it constantly sets for [each of us].”<sup>24</sup>

Finding meaning, according to Frankl’s theory, involves “perceiving a possibility embedded in reality” and searching for challenging tasks “whose completion might add meaning to [one’s] existence.” But such perceiving and searching are frustrated by the focus on self: “As long as modern literature confines itself to, and contents itself with, self-expression—not to say self-exhibition—it reflects its authors’ sense of futility and absurdity. What is more important, it also creates absurdity. This is understandable in light of the fact that meaning must be discovered, it cannot be invented. Sense cannot be created, but what may well be created is nonsense.”<sup>25</sup>

Whether we agree completely with Frankl, one thing is clear: Contemporary American culture would be markedly different if the emphasis over the past several decades had been on Frankl’s theory rather than on the theories of Maslow and the other humanistic psychologists. All of us would have been affected—we can only imagine how profoundly—in our attitudes, values, and beliefs.

### Becoming an Individual

In light of what we have discussed, we should regard individuality not as something we are born with but rather as something acquired—or, more precisely, earned. Individuality begins in the realization that it is impossible to escape being influenced by other people and by circumstance. The essence of individuality is vigilance. The following guidelines will help you achieve this:

1. Treat your first reaction to any person, issue, or situation as tentative. No matter how appealing it may be, refuse to embrace it until you have examined it.
2. Decide why you reacted as you did. Consider whether you borrowed the reaction from someone else—a parent or friend, perhaps, or a celebrity or fictional character on television. If possible, determine what specific experiences conditioned you to react this way.
3. Think of other possible reactions you might have had to the person, issue, or situation.
4. Ask yourself whether one of the other reactions is more appropriate than your first reaction. And when you answer, resist the influence of your conditioning.

To ensure that you will really be an individual and not merely claim to be one, apply these guidelines throughout your work in this book, as well as in your everyday life.

#### Applications

Note: One of the best ways to develop your thinking (and writing) skills is to record your observations, questions, and ideas in a journal and then, as time permits, to reflect on what you have recorded—considering the meaning and application of the observations, answering the questions, elaborating on the ideas (and, where appropriate, challenging them), and recording your insights. An inexpensive bound notebook or spiral notebook will serve the purpose. A good approach is to record your initial observations, questions, and ideas on the left side of the page, leaving the right side blank for your later analysis and commentary. The value of this reflective process is so great that you should consider keeping such a journal even if your instructor does not make it a formal part of the course.

1. Do a brief study of attention shifts such as the one described in the chapter. Record a half-hour show. Then play the show back twice, the first time counting the number of shifts within the program, excluding commercials, and the second time counting only those within commercials. Complete the necessary arithmetic and be prepared to share your results in class.
2. Reflect on your findings in application 1.Write several paragraphs discussing the implications of those findings for education, business, and family life.
3. Many people cheerfully pay $6 or $7 a gallon for designer drinking water but moan and groan when they have to pay $3 a gallon for gasoline. Does anything you read in this chapter help you understand why this is so?
4. Imagine how different America might beif Frankl’s emphasis on self-transcendence and personal responsibility, rather than Maslow’s emphasis on self-actualization and popular culture’s emphasis on self-esteem, had been dominant for the past fifty years. List as many ways as you can in which our society might be different today and comment on whether each would be beneficial or harmful. Be prepared to explain your views in class discussion.
5. Watch one of the music video channels—MTV,VH1,CMT,BET—for at least an hour. Analyze how men and women are depicted in the videos. Note significant details. For example, observe whether men are depicted in power roles more than women and whether women are portrayed as objects of male desire. Decide what attitudes and values are conveyed. (You might want to record as you are watching so that you can review what you have seen, freeze significant frames for closer analysis, and keep your observations for later reference or class viewing and discussion.)
6. Suppose you asked a friend, “How did you acquire your particular identity—your sentiments and preferences and attitudes?” Then suppose the friend responded, “I’m an individual. No one else influences me. I do my own thing, and I select the sentiments and preferences and attitudes that suit me.” How would you explain to your friend what you learned in this chapter?
7. Ask yourself the question,Who am I? Write down ten answers to this question, each on a separate slip of paper. Use the first three paragraphs of this chapter to help you frame your answers. Arrange the pieces of paper in order of their importance to you. Then explain the arrangement—that is, which selfdescriptions are most important to you, and why?
8. Identify the various positive and negative influences that have shaped you. Be sure to include the particular as well as the general and the subtle as well as the obvious influences. Which of those influences have had the greatest effect on you? Explain the effects as precisely as you can.
9. Note your immediate reaction to each of the following statements. Then apply the four guidelines given in this chapter for achieving individuality.
    a. Health care workers should be required to be tested for HIV/AIDS.
    b. Beauty contests and talent competitions for children should be banned.
    c. Extremist groups like the Ku Klux Klan should be allowed to hold rallies on public property or be issued permits to hold parades on city streets.
    d. Freshman composition should be a required course for all students.
    e. High school and college athletes should be tested for anabolic steroid use.
    f. Creationism should be taught in high school biology classes.
    g. Polygamy should be legalized.
    h. The voting age should be lowered to sixteen.
    i. The prison system should give greater emphasis to the punishment of inmates than to their rehabilitation.
    j. Doctors and clinics should be required to notify parents of minors when they prescribe birth control devices or facilitate abortions for the minors.
    k. A man’s self-esteem is severely injured if his wife makes more money than he makes.
    l. Women like being dependent on men.
10. Group discussion exercise: Discuss several of the statements in application 9 with two or three of your classmates, applying the four guidelines presented in this chapter for developing individuality. Be prepared to share your group’s ideas with the class.

#### A Difference of Opinion

The following passage summarizes an important difference of opinion. After reading the statement, use the library and/or the Internet and find what knowledgeable people have said about the issue. Be sure to cover the entire range of views. Then assess the strengths and weaknesses of each. If you conclude that one view is entirely correct and the others are mistaken, explain how you reached that conclusion. If, as is more likely, you find that one view is more insightful than the others but that they all make some valid points, construct a view of your own that combines insights from all views and explain why that view is the most reasonable of all. Present your response in a composition or an oral report, as your instructor specifies.

> Should captured terrorists be tried in military or criminal courts? When the United States decided to use the military base at Guantanamo Bay, Cuba, to detain individuals captured on the battlefield in the Iraq war, many people protested the decision. Some argued that captured individuals should be considered criminals rather than prisoners of war and accorded the rights guaranteed by the U.S. Constitution to all people accused of crimes. Others argued for classifying the individuals as prisoners of war and treating them as specified in the Geneva Conventions of 1949. Supporters of the government’s decision reject both arguments, contending that captured terrorists are neither criminals nor soldiers but “unlawful combatants,” adding that any other designation would impose burdens on the United States that would make it difficult to fight terrorism and thereby threaten national security.

Begin your analysis by conducting a Google search using the term “status captured terrorists.”

## What Is Critical Thinking?

When Arthur was in the first grade, the teacher directed the class to “think.” “Now, class,” she said, “I know this problem is a little harder than the ones we’ve been doing, but I’m going to give you a few extra minutes to think about it. Now start thinking.”

It was not the first time Arthur had heard the word used. He’d heard it many times at home, but never quite this way. The teacher seemed to be asking for some special activity, something he should know how to start and stop—like his father’s car. “Vroom-m-m,” he muttered half aloud. Because of his confusion, he was unaware he was making the noise.

“Arthur, please stop making noises and start thinking.”

Embarrassed and not knowing quite what to do, he looked down at his desk. Then, out of the corner of his eye, he noticed that the little girl next to him was staring at the ceiling. “Maybe that’s the way you start thinking,” he guessed. He decided the others had probably learned how to do it last year, that time he was home with the measles. So he stared at the ceiling.

As he progressed through grade school and high school, he heard that same direction hundreds of times. “No, that’s not the answer, you’re not thinking—now think!” And occasionally he would hear from particularly self-pitying teachers given to muttering to themselves aloud: “What did I do to deserve this? Don’t they teach them anything in the grades anymore? Don’t you people care about ideas? Think, dammit, THINK.”

So Arthur learned to feel somewhat guilty about the whole matter. Obviously, this thinking was an important activity that he’d failed to learn. Maybe he lacked the brain power. But he was resourceful enough. He watched the other students and did what they did. Whenever a teacher started in about thinking, he screwed up his face, furrowed his brow, scratched his head, stroked his chin, stared off into space or up at the ceiling, and repeated silently to himself, “Let’s see now, I’ve got to think about that, think, think—I hope he doesn’t call on me—think.” Though Arthur didn’t know it, that’s just what the other students were saying to themselves.

Your experience may have been similar to Arthur’s. In other words, many people may have simply told you to think without ever explaining what thinking is and what qualities a good thinker has that a poor thinker lacks. If that is the case, you have a lot of company. Extensive, effective training in thinking is the exception rather than the rule. This fact and its unfortunate consequences are suggested by the following comments from accomplished observers of the human condition:

> The most interesting and astounding contradiction in life is to me the constant insistence by nearly all people upon “logic,” “logical reasoning,” “sound reasoning,” on the one hand, and on the other their inability to display it, and their unwillingness to accept it when displayed by others.<sup>1</sup>
> Most of our so-called reasoning consists in finding arguments for going on believing as we already do.<sup>2</sup>
> Clear thinking is a very rare thing, but even just plain thinking is almost as rare. Most of us most of the time do not think at all. We believe and we feel, but we do not think.<sup>3</sup>
> Mental indolence is one of the commonest of human traits.<sup>4</sup>

What is this activity that everyone claims is important but few people have mastered? Thinking is a general term used to cover numerous activities, from daydreaming to reflection and analysis. Here are just some of the synonyms listed in Roget’s Thesaurus for think:

appreciate believe cerebrate cogitate conceive consider
consult contemplate deliberate digest discuss dream
fancy imagine meditate muse ponder realize
reason reflect ruminate speculate suppose weigh

All of those are just the names that thinking goes under. They really don’t explain it. The fact is, after thousands of years of humans’ experiencing thought and talking and writing about thinking, it remains in many respects one of the great mysteries of our existence. Still, though much is yet to be learned, a great deal is already known.

### Mind, Brain, or Both?

Most modern researchers use the word mind synonymously with brain, as if the physical organ that resides in the human skull were solely responsible for thinking. This practice conveniently presupposes that a problem that has challenged the greatest thinkers for millennia—the relationship between mind and physical matter—was somehow solved when no one was looking. The problem itself and the individuals who spent their lives wrestling with it deserve better.

Neuroscience has provided a number of valuable insights into the cognitive or thinking activities of the brain. It has documented that the left hemisphere of the brain deals mainly with detailed language processing and is associated with analysis and logical thinking, that the right hemisphere deals mainly with sensory images and is associated with intuition and creative thinking, and that the small bundle of nerves that lies between the hemispheres—the corpus callosum—integrates the various functions.

The research that produced these insights showed that the brain is necessary for thought, but it has not shown that the brain is sufficient for thought. In fact, many philosophers claim it can never show that. They argue that the mind and the brain are demonstrably different. Whereas the brain is a physical entity composed of matter and therefore subject to decay, the mind is a metaphysical entity. Examine brain cells under the most powerful microscope and you will never see an idea or concept— for example, beauty, government, equality, or love—because ideas and concepts are not material entities and so have no physical dimension. Where, then, do these nonmaterial things reside? In the nonmaterial mind.<sup>5</sup>

The late American philosopher William Barrett observed that “history is, fundamentally, the adventure of human consciousness” and “the fundamental history of humankind is the history of mind.” In his view, “one of the supreme ironies of modern history” is the fact that science, which owes its very existence to the human mind, has had the audacity to deny the reality of the mind. As he put it, “the offspring denies the parent.”<sup>6</sup>

The argument over whether the mind is a reality is not the only issue about the mind that has been hotly debated over the centuries. One especially important issue is whether the mind is passive, a blank slate on which experience writes, as John Locke held, or active, a vehicle by which we take the initiative and exercise our free will, as G. W. Leibnitz argued. This book is based on the latter view.

### Critical Thinking Defined

Let’s begin by making the important distinction between thinking and feeling. I feel and I think are sometimes used interchangeably, but that practice causes confusion. Feeling is a subjective response that reflects emotion, sentiment, or desire; it generally occurs spontaneously rather than through a conscious mental act. We don’t have to employ our minds to feel angry when we are insulted, afraid when we are threatened, or compassionate when we see a picture of a starving child. The feelings arise automatically.

Feeling is useful in directing our attention to matters we should think about; it also can provide the enthusiasm and commitment necessary to complete arduous mental tasks. However, feeling is never a good substitute for thinking because it is notoriously unreliable. Some feelings are beneficial, honorable, even noble; others are not, as everyday experience demonstrates. We often feel like doing things that will harm us—for example, smoking, sunbathing without sunscreen, telling off our professor or employer, or spending the rent money on lottery tickets.

Zinedine Zidane was one of the greatest soccer players of his generation, and many experts believed that in his final season (2006) he would lead France to the pinnacle of soccer success—winning the coveted World Cup. But then, toward the end of the championship game against Italy, he viciously head-butted an Italian player in full view of hundreds of millions of people. The referee banished him from the field, France lost the match, and a single surrender to feeling forever stained the brilliant career Zidane had dedicated his life to building.

In contrast to feeling, thinking is a conscious mental process performed to solve a problem, make a decision, or gain understanding. Whereas feeling has no purpose beyond expressing itself, thinking aims beyond itself to knowledge or action. This is not to say that thinking is infallible; in fact, a good part of this book is devoted to exposing errors in thinking and showing you how to avoid them. Yet for all its shortcomings, thinking is the most reliable guide to action we humans possess. To sum up the relationship between feeling and thinking, feelings need to be tested before being trusted, and thinking is the most reasonable and reliable way to test them.

There are three broad categories of thinking: reflective, creative, and critical. The focus of this book is on critical thinking. The essence of critical thinking is evaluation. Critical thinking, therefore, may be defined as the process by which we test claims and arguments and determine which have merit and which do not. In other words, critical thinking is a search for answers, a quest. Not surprisingly, one of the most important techniques used in critical thinking is asking probing questions. Where the uncritical accept their first thoughts and other people’s statements at face value, critical thinkers challenge all ideas in this manner:

Thought
1. Professor Vile cheated me in my composition grade. He weighted some themes more heavily than others.
2. Before women entered the work force, there were fewer divorces. That shows that a woman’s place is in the home.
3. A college education isn’t worth what you pay for it. Some people never reach a salary level appreciably higher than the level they would have reached without the degree.
Question
1. Did he grade everyone on the same standard? Were the different weightings justified?
2. How do you know that this factor, and not some other one(s), is responsible for the increase in divorces?
3. Is money the only measure of the worth of an education? What about increased understanding of self and life and increased ability to cope with challenges?

Critical thinking also employs questions to analyze issues. Consider, for example, the subject of values. When it is being discussed, some people say, “Our country has lost its traditional values” and “There would be less crime, especially violent crime, if parents and teachers emphasized moral values.” Critical thinking would prompt us to ask,

1. What is the relationship between values and beliefs? Between values and convictions?
2. Are all values valuable?
3. How aware is the average person of his or her values? Is it possible that many people deceive themselves about their real values?
4. Where do one’s values originate? Within the individual or outside? In thought or in feeling?
5. Does education change a person’s values? If so, is this change always for the better?
6. Should parents and teachers attempt to shape children’s values?

### Characteristics of Critical Thinkers

A number of misconceptions exist about critical thinking. One is that being able to support beliefs with reasons makes one a critical thinker. Virtually everyone has reasons, however weak they may be. The test of critical thinking is whether the reasons are good and sufficient.

Another misconception is that critical thinkers never imitate others in thought or action. If that were the case, then every eccentric would be a critical thinker. Critical thinking means making sound decisions, regardless of how common or uncommon those decisions are.

It is also a misconception that critical thinking is synonymous with having a lot of right answers in one’s head. There’s nothing wrong with having right answers, of course. But critical thinking involves the process of finding answers when they are not so readily available.

And yet another misconception is that critical thinking cannot be learned, that one either has it or does not. On the contrary, critical thinking is a matter of habit. The most careless, sloppy thinker can become a critical thinker by developing the characteristics of a critical thinker. This is not to say that all people have equal thinking potential but rather that everyone can achieve dramatic improvement.

We have already noted one characteristic of critical thinkers—skill in asking appropriate questions. Another is control of one’s mental activities. John Dewey once observed that more of our time than most of us care to admit is spent “trifling with mental pictures, random recollections, pleasant but unfounded hopes, flitting, half-developed impressions.”7 Good thinkers are no exception. However, they have learned better than poor thinkers how to stop that casual, semiconscious drift of images when they wish and how to fix their minds on one specific matter, examine it carefully, and form a judgment about it. They have learned, in other words, how to take charge of their thoughts, to use their minds actively as well as passively.

Here are some additional characteristics of critical thinkers, as contrasted with those of uncritical thinkers:

Critical Thinkers . . .
1. Are honest with themselves, acknowledging what they don’t know, recognizing their limitations, and being watchful of their own errors.
2. Regard problems and controversial issues as exciting challenges.
3. Strive for understanding, keep curiosity alive, remain patient with complexity, and are ready to invest time to overcome confusion.
4. Base judgments on evidence rather than personal preferences, deferring judgment whenever evidence is insufficient. They revise judgments when new evidence reveals error.
5. Are interested in other people’s ideas and so are willing to read and listen attentively, even when they tend to disagree with the other person.
6. Recognize that extreme views (whether conservative or liberal) are seldom correct, so they avoid them, practice fairmindedness, and seek a balanced view.
7. Practice restraint, controlling their feelings rather than being controlled by them, and thinking before acting.

Uncritical Thinkers . . .
1. Pretend they know more than they do, ignore their limitations, and assume their views are error-free.
2. Regard problems and controversial issues as nuisances or threats to their ego.
3. Are impatient with complexity and thus would rather remain confused than make the effort to understand.
4. Base judgments on first impressions and gut reactions. They are unconcerned about the amount or quality of evidence and cling to their views steadfastly.
5. Are preoccupied with themselves and their own opinions and so are unwilling to pay attention to others’ views. At the first sign of disagreement, they tend to think, “How can I refute this?”
6. Ignore the need for balance and give preference to views that support their established views.
7. Tend to follow their feelings and act impulsively.

As the desirable qualities suggest, critical thinking depends on mental discipline. Effective thinkers exert control over their mental life, direct their thoughts rather than being directed by them, and withhold their endorsement of any idea—even their own—until they have tested and confirmed it. John Dewey equated this mental discipline with freedom. That is, he argued that people who do not have it are not free persons but slaves to whim or circumstance:

> If a man’s actions are not guided by thoughtful conclusions, then they are guided by inconsiderate impulse, unbalanced appetite, caprice, or the circumstances of the moment. To cultivate unhindered, unreflective external activity is to foster enslavement, for it leaves the person at the mercy of appetite, sense, and circumstance.<sup>8</sup>

### The Role of Intuition

Intuition is commonly defined as immediate perception or comprehension of something—that is, sensing or understanding something without the use of reasoning. Some everyday experiences seem to support this definition. You may have met a stranger and instantly “known” that you would be partners for life. When a car salesman told you that the price he was quoting you was his final, rock-bottom price, your intuition may have told you he was lying. On the first day of a particular course, you may have had a strong sense that you would not do well in it.

Some important discoveries seem to have occurred instantaneously. For example, the German chemist Kekule found the solution to a difficult chemical problem intuitively. He was very tired when he slipped into a daydream. The image of a snake swallowing its tail came to him—and that provided the clue to the structure of the benzene molecule, which is a ring, rather than a chain, of atoms.<sup>9</sup> The German writer Goethe had been experiencing great difficulty organizing a large mass of material for one of his works when he learned of the tragic suicide of a close friend. At that very instant, the plan for organizing his material occurred to him in detail.<sup>10</sup> The English writer Samuel Taylor Coleridge (you may have read his Rime of the Ancient Mariner in high school) awoke from a dream with 200–300 lines of a new and complex poem clearly in mind.

Such examples seem to suggest that intuition is very different from reasoning and is not influenced by it. But before accepting that conclusion, consider these facts:

> Breakthrough ideas favor trained, active minds. It is unusual for someone totally untrained in a subject to make a significant new discovery about it. Thus, if Kekule had been a plumber, Goethe a bookkeeper, and Coleridge a hairdresser, they would almost certainly not have received the intuitions for which they are famous.
> Some intuitions eventually prove to be mistaken. That attractive stranger may turn out to be not your lifelong partner but a person for whom you develop a strong dislike. The car salesman’s final price may have proved to be exactly that. And instead of doing poorly in that course, you may have done well.
> It is difficult to make an overall assessment of the quality of our intuitions because we tend to forget the ones that prove mistaken in much the same way a gambler forgets his losses.

These facts have led some scholars to conclude that intuition is simply a consequence of thinking. They would say that something about the stranger appealed to you, something the salesman said or did suggested insincerity, something about the professor frightened you. In each case, they would explain, you made a quick decision—so quick, in fact, that you were unaware that you’d been thinking. In the case of the breakthrough ideas, the scholars would say that when people become engrossed in problems or issues, their unconscious minds often continue working on them long after they have turned their attention elsewhere. Thus, when an insight seems to come “out of nowhere,” it is actually a delayed result of thinking.

Which view of intuitions is the correct one? Are intuitions different from and independent of thinking or not? Perhaps, for now, the most prudent answer is that sometimes they are independent and sometimes they are not; we can’t be sure when they are, and therefore it is imprudent to rely on them.

### Basic Activities in Critical Thinking

The basic activities in critical thinking are investigation, interpretation, and judgment, in that order. The following chart summarizes each activity in relation to the other two.

Investigation
    Definition: Finding evidence—that is, data that will answer key questions about the issue
    Requirements: The evidence must be both relevant and sufficient.
Interpretation
    Definition: Deciding what the evidence means
    Requirements: The interpretation must be more reasonable than competing interpretations.
Judgment
    Definition: Reaching a conclusion about the issue
    Requirements: The conclusion must meet the test of logic.

As we noted previously, irresponsible thinkers first choose their conclusions and then seek out evidence to justify their choices. They fail to realize that the only conclusion worth drawing is one based on a thorough understanding of the problem or issue and its possible solutions or resolutions. Is it acceptable to speculate, guess, and form hunches and hypotheses? Absolutely. Such activities provide a helpful starting point for the thinking process. (Besides, we couldn’t avoid doing so even if we tried.) The crucial thing is not to let hunches and hypotheses manipulate our thinking and dictate our conclusion in advance.

### Critical Thinking and Writing

Writing may be used for either of two broad purposes: to discover ideas or to communicate them. Most of the writing you have done in school is undoubtedly the latter kind. But the former can be very helpful, not only in sorting out ideas you’ve already produced, but also in stimulating the flow of new ideas. For some reason, the very act of writing down one idea seems to generate additional ideas.

Whenever you write to discover ideas, focus on the issue you are examining and record all your thoughts, questions, and assertions. Don’t worry about organization or correctness. If ideas come slowly, be patient. If they come suddenly, in a rush, don’t try to slow down the process and develop any one of them; simply jot them all down. (There will be time for elaboration and correction later.) Direct your mind’s effort, but be sensitive to ideas on the fringe of consciousness. Often they, too, will prove valuable.

If you have done your discovery writing well and have thought critically about the ideas you have produced, the task of writing to communicate will be easier and more enjoyable. You will have many more ideas—carefully evaluated ones—to develop and organize.

### Critical Thinking and Discussion<sup>11</sup>

At its best, discussion deepens understanding and promotes problem solving and decision making. At its worst, it frays nerves, creates animosity, and leaves important issues unresolved. Unfortunately, the most prominent models for discussion in contemporary culture—radio and TV talk shows—often produce the latter effects.

Many hosts demand that their guests answer complex questions with simple “yes” or “no” answers. If the guests respond that way, they are attacked for oversimplifying. If, instead, they try to offer a balanced answer, the host shouts, “You’re not answering the question,” and proceeds to answer it himself. Guests who agree with the host are treated warmly; others are dismissed as ignorant or dishonest. Often as not, when two guests are debating, each takes a turn interrupting while the other shouts, “Let me finish.” Neither shows any desire to learn from the other. Typically, as the show draws to a close, the host thanks the participants for a “vigorous debate” and promises the audience more of the same next time.

Here are some simple guidelines for ensuring that the discussions you engage in—in the classroom, on the job, or at home—are more civil, meaningful, and productive than what you see on TV. By following these guidelines, you will set a good example for the people around you.

*Whenever possible, prepare in advance.* Not every discussion can be prepared for in advance, but many can. An agenda is usually circulated several days before a business or committee meeting. In college courses, the assignment schedule provides a reliable indication of what will be discussed in class on a given day. Use this information to prepare: Begin by reflecting on what you already know about the topic. Then decide how you can expand your knowledge and devote some time to doing so. (Fifteen or twenty minutes of focused searching in the library or on the Internet can produce a significant amount of information on almost any subject.) Try to anticipate the different points of view that might be expressed in the discussion and consider the relative merits of each. Keep your conclusions tentative at this point, so that you will be open to the facts and interpretations others will present.

*Set reasonable expectations.* Have you ever left a discussion disappointed that others hadn’t abandoned their views and embraced yours? Have you ever felt offended when someone disagreed with you or asked you what evidence you had to support your opinion? If the answer to either question is yes, you probably expect too much of others. People seldom change their minds easily or quickly, particularly in the case of long-held convictions. And when they encounter ideas that differ from their own, they naturally want to know what evidence supports those ideas. Expect to have your ideas questioned, and be cheerful and gracious in responding.

*Leave egotism and personal agendas at the door.* To be productive, discussion requires an atmosphere of mutual respect and civility. Egotism produces disrespectful attitudes toward others—notably, “I’m more important than other people,” “My ideas are better than anyone else’s,” and “Rules don’t apply to me.” Personal agendas, such as dislike for another participant or excessive zeal for a point of view, can lead to personal attacks and unwillingness to listen to others’ views.

*Contribute but don’t dominate.* If you are the kind of person who loves to talk and has a lot to say, you probably contribute more to discussions than other participants. On the other hand, if you are more reserved, you may seldom say anything. There is nothing wrong with being either kind of person. However, discussions tend to be most productive when everyone contributes ideas. For this to happen, loquacious people need to exercise a little restraint, and more reserved people need to accept responsibility for sharing their thoughts.

*Avoid distracting speech mannerisms.* Such mannerisms include starting one sentence and then abruptly switching to another; mumbling or slurring your words; and punctuating every phrase or clause with audible pauses (“um,” “ah,”) or meaningless expressions (“like,” “you know,” “man”). These annoying mannerisms distract people from your message. To overcome them, listen to yourself when you speak. Even better, tape your conversations with friends and family (with their permission), then play the tape back and listen to yourself. Whenever you are engaged in a discussion, aim for clarity, directness, and economy of expression.

*Listen actively.* When the participants don’t listen to one another, discussion becomes little more than serial monologue—each person taking a turn at speaking while the rest ignore what is being said. This can happen quite unintentionally because the mind can process ideas faster than the fastest speaker can deliver them. Your mind may get tired of waiting and wander about aimlessly like a dog off its leash. In such cases, instead of listening to the speaker’s words, you may think about her clothing or hairstyle or look outside the window and observe what is happening there. Even when you make a serious effort to listen, it is easy to lose focus. If the speaker’s words trigger an unrelated memory, you may slip away to that earlier time and place. If the speaker says something you disagree with, you may begin framing a reply. The best way to maintain your attention is to be alert for such distractions and to resist them. Strive to enter the speaker’s frame of mind, understand what is said, and connect it with what was said previously. Whenever you realize your mind is wandering, drag it back to the task.

*Judge ideas responsibly.* Ideas range in quality from profound to ridiculous, helpful to harmful, ennobling to degrading. It is therefore appropriate to pass judgment on them. However, fairness demands that you base your judgment on thoughtful consideration of the overall strengths and weaknesses of the ideas, not on initial impressions or feelings. Be especially careful with ideas that are unfamiliar or different from your own because those are the ones you will be most inclined to deny a fair hearing.

*Resist the urge to shout or interrupt.* No doubt you understand that shouting and interrupting are rude and disrespectful behaviors, but do you realize that in many cases they are also a sign of intellectual insecurity? It’s true. If you really believe your ideas are sound, you will have no need to raise your voice or to silence the other person. Even if the other person resorts to such behavior, the best way to demonstrate confidence and character is by refusing to reciprocate. Make it your rule to disagree without being disagreeable.

### Avoiding Plagiarism<sup>12</sup>

Once ideas are put into words and published, they become intellectual property, and the author has the same rights over them as he or she has over a material possession such as a house or a car. The only real difference is that intellectual property is purchased with mental effort rather than money. Anyone who has ever wracked his or her brain trying to solve a problem or trying to put an idea into clear and meaningful words can appreciate how difficult mental effort can be.

Plagiarism is passing off other people’s ideas or words as one’s own. It is doubly offensive in that it both steals and deceives. In the academic world, plagiarism is considered an ethical violation and is punished by a failing grade for a paper or a course or even by dismissal from the institution. Outside the academy, it is a crime that can be prosecuted if the person to whom the ideas and words belong wishes to bring charges. Either way, the offender suffers dishonor and disgrace, as the following examples illustrate:

* When a university in South Africa learned that professor Marks Chabel had plagiarized most of his doctoral dissertation from Kimberly Lanegran of the University of Florida, the university fired Chabel. Moreover, the university that had awarded him his Ph.D. revoked it.
* When U.S.Senator Joseph Biden was seeking the 1988 Democratic presidential nomination, it was revealed that he had plagiarized passages from speeches by British politician Neil Kinnock and by Robert Kennedy. It was also learned that, while in law school, he had plagiarized a number of pages from a legal article. The ensuing scandal led Biden to withdraw his candidacy and has continued to stain his reputation.
*  The reputation of historian Stephen Ambrose was tarnished by allegations that over the years he plagiarized the work of several authors. Doris Kearns Goodwin, historian and advisor to President Lyndon Johnson, suffered a similar embarrassment when she was discovered to have plagiarized from more than one source in one of her books.
* When James A. Mackay, a Scottish historian, published a biography of Alexander Graham Bell in 1998, Robert Bruce presented evidence that the book was largely plagiarized from his 1973 biography, which had won a Pulitzer Prize. Mackay was forced to withdraw his book from the market. (Incredibly, he did not learn from the experience because he then published a biography of John Paul Jones, which was plagiarized from a 1942 book by Samuel Eliot Morison.)
* When New York Times reporter Jason Blair was discovered to have plagiarized stories from other reporters and fabricated quotations and details in his stories, he resigned his position in disgrace. Soon afterward, the two senior editors who had been his closest mentors also resigned, reportedly because of their irresponsible handling of Blair’s reportage and the subsequent scandal.

Some cases of plagiarism are attributable to intentional dishonesty, others to carelessness. But many, perhaps most, are due to misunderstanding.  The instructions “Base your paper on research rather than on your own unfounded opinions” and “Don’t present other people’s ideas as your own” seem contradictory and may confuse students, especially if no clarification is offered. Fortunately, there is a way to honor both instructions and, in the process, to avoid plagiarism.

Step 1: When you are researching a topic, keep your sources’ ideas separate from your own. Begin by keeping a record of each source of information you consult. For an Internet source, record the Web site address, the author and title of the item, and the date you visited the site.  For a book, record the author, title, place of publication, publisher, and date of publication. For a magazine or journal article, record the author, title, the name of the publication, and its date of issue. For a TV or radio broadcast, record the program title, station, and date of transmission.

Step 2: As you read each source, note the ideas you want to refer to in your writing. If the author’s words are unusually clear and concise, copy them exactly and put quotation marks around them. Otherwise, paraphrase— that is, restate the author’s ideas in your own words. Write down the number(s) of the page(s) on which the author’s passage appears.

If the author’s idea triggers a response in your mind—such as a question, a connection between this idea and something else you’ve read, or an experience of your own that supports or challenges what the author says—write it down and put brackets (not parentheses) around it so that you will be able to identify it as your own when you review your notes.  Here is a sample research record illustrating these two steps:

> Adler, Mortimer J. The Great Ideas: A Lexicon of Western Thought (New York: Macmillan, 1992) Says that throughout the ages, from ancient Greece, philosophers have argued about whether various ideas are true. Says it’s remarkable that most renowned thinkers have agreed about what truth is—”a correspondence between thought and reality.” 867 Also says that Freud saw this as the scientific view of truth. Quotes Freud: “This correspondence with the real external world we call truth. It is the aim of scientific work, even when the practical value of that work does not interest us.” 869 [I say true statements fit the facts; false statements do not.]

Whenever you look back on this record, even a year from now, you will be able to tell at a glance which ideas and words are the author’s and which are yours. The first three sentences are, with the exception of the directly quoted part, paraphrases of the author’s ideas. Next is a direct quotation. The final sentence, in brackets, is your own idea.

Step 3: When you compose your paper, work borrowed ideas and words into your own writing by judicious use of quoting and paraphrasing.  In addition, give credit to the various authors. Your goal here is to eliminate all doubt about which ideas and words belong to whom. In formal presentations, this crediting is done in footnotes; in informal ones, it is done simply by mentioning the author’s name.

Here is an example of how the material from Mortimer Adler might be worked into a composition. (Note the form that is used for the footnote.) The second paragraph illustrates how your own idea might be expanded:

> Mortimer J. Adler explains that throughout the ages, from the time of the ancient Greeks, philosophers have argued about whether various ideas are true. But to Adler the remarkable thing is that, even as they argued, most renowned thinkers have agreed about what truth is. They saw it as “a correspondence between thought and reality.” Adler points out that Sigmund Freud believed this was also the scientific view of truth. He quotes Freud as follows: “This correspondence with the real external world we call truth. It is the aim of scientific work, even when the practical value of that work does not interest us.”*

> This correspondence view of truth is consistent with the commonsense rule that a statement is true if it fits the facts and false if it does not. For example, the statement “The twin towers of New York’s World Trade Center were destroyed on September 11, 2002,” is false because they were destroyed the previous year. I may sincerely believe that it is true, but my believing in no way affects the truth of the matter. In much the same way, if an innocent man is convicted of a crime, neither the court’s decision nor the world’s acceptance of it will make him any less innocent. We may be free to think what we wish, but our thinking can’t alter reality

> *Mortimer J. Adler, The Great Ideas: A Lexicon of Western Thought (New York: Macmillan, 1992), pp. 867, 869.

#### Applications

1. Think back on your previous schooling. How closely has your experience matched Arthur’s? Explain.
2. Reflect on your powers of concentration. Do you find it difficult to ponder important matters? Are you able to prevent the casual, semiconscious drift of images from interrupting your thoughts? Do you have less control in some situations than in others? Explain.
3. Rate yourself on each of the eight characteristics of good critical thinkers that are listed on pp. 24–26. Which are you strongest in? Which weakest? If your behavior varies from situation to situation, try to determine what kinds of issues or circumstances bring out your best and worst mental qualities.
4. Consider how you approach problems and issues. Is there any pattern to the way you think about a problem or an issue? Does an image come to mind first? Or perhaps a word? What comes next? And what after that? If you can’t answer these questions completely, do this exercise: Flip half a dozen pages ahead in this book, pick a sentence at random, read it, and note how your mind deals with it. (Such thinking about your thinking may be a little awkward at first. If it is, try the exercise two or three times.)
5. Read each of the following statements carefully. Then decide what question(s), if any, a good critical thinker would find it appropriate to ask.
a. Television news sensationalizes its treatment of war because it gives us pictures only of injury, death, and destruction.
b. My parents were too strict—they wouldn’t let me date until I was sixteen.
c. It’s clear to me that Ralph doesn’t care for me—he never speaks when we pass in the hall.
d. From a commercial for a news network: “The news is changing every minute of the day, so you constantly need updating to keep you informed.”
e. The statement of an Alabama public elementary school teacher who had students recite the Lord’s Prayer and say grace before meals: “I feel part of my job as a teacher is to instill values children need to have a good life.”

#### A Difference of Opinion

The following passage summarizes an important difference of opinion. After reading the statement, use the library and/or the Internet and find what knowledgeable people have said about the issue. Be sure to cover the entire range of views. Then assess the strengths and weaknesses of each. If you conclude that one view is entirely correct and the others are mistaken, explain how you reached that conclusion. If, as is more likely, you find that one view is more insightful than the others but that they all make some valid points, construct a view of your own that combines the insights from all views and explain why that view is the most reasonable of all. Present your response in a composition or an oral report, as your instructor specifies.

> What response should the United States make to the problem of illegal immigration? As violence on the southern U.S. border increases and illegal entry continues, many Americans are becoming impatient with the federal government’s failure to solve the border problem. The state of Arizona has already taken action to apprehend illegals but has been criticized for interfering in matters under federal jurisdiction. Is Arizona’s approach the most reasonable one? If not, what approach would be? 

Begin your analysis by conducting a Google search using the terms “Arizona illegal immigrants” and “border security issues.”

## What Is Truth?

For hundreds of years, philosophers battled over whether “truth” exists. The argument usually concerned Truth with a capital T, a kind of complete record of whatever was, is, or will be, error-proof, beyond doubt and dispute, a final test of the rightness or wrongness of people’s ideas and theories.

Those who accepted the existence of this Truth believed it was a spiritual reality, not a physical one. That is, it was not a celestial ledger or file drawer—yet it was beyond time and space. It was considered an understanding among the gods, or an idea in the mind of God, or simply the sum total of Reality. Could humans ever come to know Truth? Some said, no, never. Others said, yes but only in the afterlife. Still others said that the wisest and best of humans could catch glimpses of it and that the rest of humanity could learn about it through these special ones.

Those who rejected this notion of an awesome, all-embracing Truth argued that it was an empty notion. How could all reality be summed up that way? More important, what possible evidence could be offered in support of its existence? Many who reasoned this way dismissed the idea of Truth as wishful thinking, a kind of philosophical security blanket. A few went further and denied even the existence of truths (no capital).

Our age has inherited the whole argument. The focus, however, has changed. It seldom concerns Truth anymore. Even if Truth does exist, it’s of little help to us in our world and our lives because it is beyond human understanding. Even many people of strong and rather conservative religious views no longer consider the question of Truth important to the understanding or practice of their faith.

Still, the question of truth, or even truths, remains, and the position we take toward this question does have an important bearing on how we conduct our thinking and acting. Unfortunately, there is a good deal of murkiness and confusion about the concept. The rest of this chapter will attempt to shed light on it.

It’s fashionable today to believe that truth is relative and subjective.  “Everyone creates his or her own truth,” the saying goes, “and what is true for you may not be true for me.” The meaning of this statement goes far beyond “It’s a free country and I can believe what I want.” The claim means that whatever a person thinks is true because he or she thinks it is. Not surprisingly, to challenge another person’s view on an issue is considered bad form. “That’s my truth you’re talking about, Buster. Show a little respect.”

The implications of this notion are quite staggering, yet for some reason few people acknowledge them, and fewer still are interested in testing their reasonableness. One implication is that everyone is right and no one is wrong. In fact, no one can be wrong. (What an argument this would make against objective tests—true/false, multiple choice, and so on: “My answers can’t be wrong, professor. They’re my truth!”) Another is that everyone’s perception and memory work flawlessly, with never a blunder, glitch, or gaffe. And another is that no one adopts other people’s “truths.” The idea of creating truth rules out borrowing—if truth is intensely personal, each person’s truth must be unique. Let’s examine all these ideas more closely.

### Where Does It All Begin?

The idea of creating our own truth without outside influence or assistance may sound reasonable if we focus only on our adulthood. The moment we consider our childhood, however, the idea becomes suspect, because in childhood we were all dependent in every sense: physically, emotionally, and intellectually. What we knew and believed about everything was what others told us. We asked questions—”Why, Mommy?” “Why, Daddy?” Our parents answered them. We accepted those answers and made them the foundation of our belief system, no matter how elaborate it would become in adulthood.

Relativists could, of course, claim that we leave all those early influences behind when we reach adulthood, but that denies the most fundamental principles of psychology. Here is how one writer explained the continuing influence of childhood experience:

> We are told about the world before we see it. We imagine most things before we experience them. And those preconceptions, unless education has made us acutely aware, govern deeply the whole process of perception.  They mark out certain objects as familiar or strange, emphasizing the difference, so that the slightly familiar is seen as very familiar, and the somewhat strange as sharply alien. They are aroused by small signs, which may vary from a true index to a vague analogy. Aroused, they flood fresh vision with older images, and project into the world what has been resurrected in memory.<sup>1</sup>

You have heard the old saying seeing is believing. The reverse—believing is seeing—is equally correct. To a greater or lesser extent, what we regard as our unique perspective bears the imprint of other people’s ideas and beliefs.

### Imperfect Perception

Is perception flawless? Hardly. For one thing, it is influenced by our desires, interests, and expectations: “From the outset perception is selective and tends to simplify the world around us. Memory continues and hastens the process.”<sup>2</sup> For another, even within its limited focus, perception is often flawed. A college student who is positive that the textbook contains a certain statement answers an exam question with perfect confidence. Yet when the student gets the corrected test back and finds the question marked wrong, then hurriedly flips open the book and examines the passage again, he or she may find it says something else entirely.

Moviegoers in the 1930s and 1940s were thrilled as Tarzan uttered his famous yell and swung through the treetops to catch the villain. Tell them that Tarzan never made that yell and they’ll say, “False, we heard it with our own ears.” And yet it’s not false. According to one of the men who first played the role of Tarzan, Buster Crabbe, that yell was dubbed into the films in the studio. It was a blend of three voices—a soprano’s, a baritone’s, and a hog caller’s.

At least a dozen times every weekend from September to January, the imperfection of human observation is underlined by that marvel of technology, the instant replay. Is there a football fan anywhere who doesn’t occasionally scream, “Bad call!” only to be proved wrong a moment later? We can be sure enough to bet a week’s wages that the pass receiver’s feet came down inbounds or that the running back’s knee hit the ground before the ball came loose. And then the replay shows us how erroneous our initial perception was.

The vagaries of perception have long been noted by those who deal with human testimony—notably, trial lawyers, police officers, and psychologists.  It is well established that a number of factors can make us see and hear inaccurately. Darkness, cloudy conditions, or distance from what we are witnessing may obscure our vision. We may be distracted at a crucial moment. If we are tired or in the grip of powerful emotions such as fear or anger, our normal perceptiveness may be significantly diminished.  Also, perception may be intermingled with interpretation—the expectation that an event will unfold in a certain way may color our perception of the way the event actually unfolds. Loyalty and affection toward the people or things involved may distort our vision as well. If someone we dislike speaks in a loud voice and is animated, we may regard that person as showing off to get attention. But if a friend behaves in the same way, we may regard him or her as vivacious and extroverted.

### Imperfect Memory

Even when our perception is initially flawless, our memory often distorts the data. We forget details, and when later attempting to recall what happened we resort to imagination to fill in the blanks. Though we may at first be aware that such a process of reconstruction is occurring, this awareness soon fades, and we come to believe we are remembering the original perception. As psychologist William James explained,

> The most frequent source of false memory is the accounts we give to others of our experiences. Such acts we almost always make more simple and more interesting than the truth. We quote what we should have said or done rather than what we really said or did; and in the first telling we may be fully aware of the distinction, but [before] long the fiction expels the reality from memory and [replaces it]. We think of what we wish had happened, of possible [interpretations] of acts, and soon we are unable to distinguish between things that actually happened and our own thoughts about what might have occurred. Our wishes, hopes, and sometimes fears are the controlling factor.<sup>3</sup>

As if this weren’t enough, memory is vulnerable to contamination from outside the mind. Memory expert Elizabeth Loftus showed children a one-minute film and then asked, “Did you see a bear?” or “Did you see a boat?” They remembered seeing them, even though no bears or boats were in the film. She also showed adults a film of an auto accident and then asked them about it. By using the word “smash” instead of “hit,” she was able to change the viewers’ estimate of the cars’ speed and to create a memory of broken glass where there was none. In another experiment, Loftus asked the parents of college students to describe some events from their sons’ and daughters’ childhoods. Then she talked with each student about those events but added a fake event or two. With only slight coaxing, the students “remembered” the fake events, were able to elaborate on the details, and in some cases refused to believe they were fake even when Loftus explained what she had done.<sup>4</sup>

### Deficient Information

The quality of a belief depends to a considerable extent on the quality of the information that backs it up. Because it’s a big world and reality has many faces, it’s easy for us to be misinformed. How many drivers take the wrong turn because of faulty directions? How many people get on the wrong bus or train? How many car owners put too much or too little air in their tires on the advice of some service station attendant? And, if misinformation is common enough in such relatively simple matters, how much more common is it in complex matters like law and medicine and government and religion?

It’s possible, of course, to devote a lifetime of study to a particular field. But not even those who make that kind of commitment can know everything about their subject. Things keep happening too fast. They occur whether we’re watching or not. There’s no way to turn them off when we take a coffee break or go to the bathroom. The college student who hasn’t been home in three months may be able to picture the neighbor’s elm tree vividly, yet it may have been cut down two months ago. The soldier may have total recall of his hometown—every sight and sound and smell—and return home to find half of Main Street sacrificed to urban renewal, the old high school hangout closed, and a new car in his best friend’s driveway.

### Even the Wisest Can Err

So far, we’ve established that people can be mistaken in what they perceive and remember and that the information they receive can be faulty or incomplete. But these matters concern individuals. What of group judgment—the carefully analyzed observations of the best thinkers, the wisest men and women of the time? Is that record better? Happily, it is.  But it, too, leaves a lot to be desired.

All too often, what is taken as truth one day by the most respected minds is proved erroneous the next. You undoubtedly know of some examples. In the early seventeenth century, when Galileo suggested that the sun is the center of our solar system, he was charged with heresy, imprisoned, and pressured to renounce his error. The “truth” of that time, accepted by every scientist worthy of the name, was that the earth was the center of the solar system.

Here are some other examples you may not have heard about in which the “truth” turned out not to be true:

* For a long time surgeons used talc on the rubber gloves they wore while performing surgery. Then they discovered it could be poisonous.  So they switched to starch, only to find that it, too, could have a toxic effect on surgical patients.<sup>5</sup>
* Film authorities were certain they were familiar with all the films the late Charlie Chaplin ever made. Then, in 1982, a previously unknown film was discovered in a British screen archive vault.<sup>6</sup>
* For hundreds of years historians believed that although the people of Pompeii had been trapped by the eruption of Mount Vesuvius in A.D. 79, the people of neighboring Herculaneum had escaped. Then the discovery of eighty bodies (and the hint of hundreds more) under the volcanic ash revealed that many from Herculaneum had also been trapped.<sup>7</sup>
* Your grandparents probably learned that there are eight planets in our solar system. Since Pluto was discovered in 1930, your parents and you learned there are nine. Then Joseph L. Brady of the University of California suggested there might be ten.  <sup>8</sup> But more recently Pluto was removed from the list.
* After morphine was used by doctors for some years as a painkiller, it was found to be addictive. The search began for a nonaddictive substitute. What was found to take its place? Heroin!<sup>9</sup>

